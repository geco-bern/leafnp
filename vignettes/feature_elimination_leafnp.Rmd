---
title: "Feature elimination for leafnp"
author: "Beni Stocker"
date: "9/6/2021"
output: html_document
---


```{r setup, include=FALSE}
library(tidyverse)
library(ranger)
library(caret)
library(visdat)
library(vip)
library(pdp)
library(nnet)
library(recipes)
library(knitr)
library(forcats)
# library(rbeni)
overwrite <- FALSE
```

This looks at feature elimination results and makes the selection of the most important predictors for subsequent (final) model fitting (in other Rmd files).

Obtain outputs from Euler, generated with `feature_elimination_leafnp.R`. Download CSV files into `data/`

Load data aggregated to sites, done in `randomforest_leafnp.Rmd`.
```{r}
dfs <- readRDS(paste0(here::here(), "/data/dfs_leafnp_20210729.rds"))
```

## Leaf N

### Get FE results

```{r}
target <- "leafN"
df_fe_summary <- read_csv(paste0(here::here(), "/data/df_fe_summary_", target, ".csv"))
df_fe <- read_csv(paste0(here::here(), "/data/df_fe_", target, ".csv"))
df_vip <- read_csv(paste0(here::here(), "/data/df_vip_", target, ".csv"))
```

`df_fe` provides the full information about the updated R2 after respective variable was dropped.

With this, we can re-create the feature elimination, getting name of candidate predictor at each step ("level") for which, when dropped, the model still achieved the highest R2.

```{r}
# use this, df_fe_summary was accidentally overwritten. 
df_fe_summary_reconstr <- df_fe |> 
  group_by(level)|> 
  filter(rsq == max(rsq)) |> 
  ungroup()
df_fe_summary_reconstr$pred[nrow(df_fe_summary_reconstr)] <- "ndep"
df_fe_summary_reconstr$rsq[2:nrow(df_fe_summary_reconstr)] <- df_fe_summary_reconstr$rsq[1:(nrow(df_fe_summary_reconstr)-1)]
df_fe_summary_reconstr$step <- 1:nrow(df_fe_summary_reconstr)

all_equal(df_fe_summary, df_fe_summary_reconstr)
```

`df_fe_summary` provides information about the updated R2 after respective variable was dropped.

This shows that there are negligible gains after ALSA (more generously after cwdx80). In other words, we might as well build a model with just the following predictors:
```{r}
longlist <- df_fe_summary_reconstr %>% 
  slice((nrow(.)-11):nrow(.)) %>% 
  pull(pred) %>% 
  as.character()
longlist

shortlist <- df_fe_summary_reconstr %>% 
  slice((nrow(.)-6):nrow(.)) %>% 
  pull(pred) %>% 
  as.character()
shortlist

saveRDS(shortlist, file = paste0(here::here(), "/data/shortlist_", target, ".rds"))
saveRDS(longlist, file = paste0(here::here(), "/data/longlist_", target, ".rds"))
```

```{r}
df_fe_summary_reconstr %>% 
  mutate(highlight = ifelse(pred %in% shortlist, "yes", "no")) %>% 
  mutate(pred = fct_reorder(pred, rev(step))) %>%
  ggplot(aes(pred, rsq, fill = highlight)) +
  scale_fill_manual( values = c( "yes"="#29a274ff", "no"="#777055ff" ), guide = FALSE ) +
  geom_bar(stat = "identity") +
  labs(x = "", y = expression(italic(R)^2)) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_flip() +
  theme_classic()

ggsave(paste0(here::here(), "/fig/rsq_stepwise_leafN.pdf"), width = 6, height = 8)

gga1 <- df_fe_summary_reconstr %>% 
  mutate(highlight = ifelse(pred %in% shortlist, "yes", "no")) %>% 
  mutate(pred = fct_reorder(pred, rev(step))) %>%
  tail(n = 13) %>% 
  ggplot(aes(pred, rsq, fill = highlight)) +
  scale_fill_manual(values = c( "yes"="#29a274ff", "no"="#777055ff" ), guide = FALSE ) +
  geom_bar(stat = "identity") +
  labs(title = "Leaf N", x = "", y = expression(italic(R)^2)) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_flip() +
  theme_classic()

gga1
ggsave(paste0(here::here(), "/fig/rsq_stepwise_leafN_sub.pdf"), width = 6, height = 4)
saveRDS(gga1, file = paste0(here::here(), "/data/gga1.rds"))
```


### Train final model

With just these (`longlist`), fit again a RF model.
```{r eval=FALSE}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafn.rds")
overwrite <- FALSE

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafn <- readRDS(filn)
  
} else {
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafN ~ ., data = dplyr::select(dfs, leafN, all_of(shortlist))) %>%
  
    ## impute by median as part of the recipe
    step_medianimpute(all_predictors())
    # step_impute_median(all_predictors())

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = 3, 
                            .min.node.size = 8,
                            .splitrule = "variance"
                            )
  
  set.seed(1982)
  
  mod_rf_caret_leafn <- train(
    pp,
    data            = dplyr::select(dfs, leafN, all_of(shortlist)),
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafn
  
  saveRDS(mod_rf_caret_leafn, file = filn)
}
```

#### Plot CV results

Visualise cross-validation results using results from the best tuned model.
```{r}
mod_rf_caret_leafn <- readRDS(paste0(here::here(), "/data/mod_rf_caret_leafn.rds"))

## get predicted values from cross-validation resamples, take mean across repetitions
df_cv <- mod_rf_caret_leafn$pred %>% 
  as_tibble() %>% 
  dplyr::filter(mtry == mod_rf_caret_leafn$bestTune$mtry, 
                splitrule == mod_rf_caret_leafn$bestTune$splitrule, 
                min.node.size == mod_rf_caret_leafn$bestTune$min.node.size) %>%
  separate(Resample, into = c(NA, "Fold"), sep = "old") %>% 
  dplyr::rename(idx = rowIndex)

out <- df_cv %>% 
  rbeni::analyse_modobs2("pred", "obs", type = "heat", shortsubtitle = TRUE)
gg1 <- out$gg +
  ylim(5,40) + xlim(5,40) +
  labs(x = "Predicted leaf N (mg/g)", y = "Observed leaf N (mg/g)")
gg1
saveRDS(gg1, file = paste0(here::here(), "/data/gg1.rds"))
write_csv(df_cv, file = paste0(here::here(), "/data/df_cv_leafn.csv"))
```


### Full data

Read full species-level data
```{r}
# df <- read_csv("~/data/LeafNP_tiandi/Global_total_leaf_N_P_Di/soil_property_extraction_20210323/global_leaf_NP_with_soil_property_from_HWSD_WISE_GSDE_Pmodel_Ndep_GTI_CO2_25032021.csv") %>% 
#   mutate(grass = tree_shrub_Herb == "H") 

df <- read_csv(paste0(here::here(), "~/data/LeafNP_tiandi/Global_total_leaf_N_P_Di/leafnp_data_covariates_20210702.csv")) %>% 
  mutate(grass = tree_shrub_Herb == "H")
```

Filter to use only data from species that were recorded in at least five different sites.
```{r}
# yields 398 species
use_species <- df %>% 
  dplyr::select(sitename, Species) %>% 
  distinct() %>% 
  group_by(Species) %>% 
  summarise(n = n()) %>% 
  dplyr::filter(n >= 5) %>% 
  pull(Species)

df <- df %>% 
  filter(Species %in% use_species)
```

### Train full model

```{r eval=FALSE}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafn_species.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafn <- readRDS(filn)
  
} else {
  
  ## one-hot encoding for Species identity (warning: 3700 species)
  df1h <- df %>% 
    recipe(leafN ~ ., data = dplyr::select(df, leafN, all_of(c(shortlist, "Species", "Family_New", "Genus")))) %>% 
    step_dummy(Species, one_hot = TRUE) %>% 
    step_dummy(Family_New, one_hot = TRUE) %>% 
    step_dummy(Genus, one_hot = TRUE) %>% 
    prep(training = df, retain = TRUE) %>% 
    juice()
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafN ~ ., data = df1h) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())


  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = floor((ncol(df1h)-2) / 3), 
                            .min.node.size = 30,
                            .splitrule = "variance"
                            )
  
  set.seed(1982)
  
  mod_rf_caret_leafn_species <- train(
    pp,
    data            = df1h,
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5
    # num.trees       = 2000,        # boosted for the final model
    # importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafn_species
  
  saveRDS(mod_rf_caret_leafn_species, file = filn)
}
```

### Train model based on phylogenetic info

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafn_species_phylo.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafn <- readRDS(filn)
  
} else {
  
  ## one-hot encoding for Species identity (warning: 3700 species)
  df1h_phylo <- df %>% 
    recipe(leafN ~ ., data = dplyr::select(df, leafN, all_of(c("Species", "Family_New", "Genus")))) %>% 
    step_dummy(Species, one_hot = TRUE) %>% 
    step_dummy(Family_New, one_hot = TRUE) %>% 
    step_dummy(Genus, one_hot = TRUE) %>% 
    prep(training = df, retain = TRUE) %>% 
    juice()
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafN ~ ., data = df1h_phylo)

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = floor((ncol(df1h_phylo)-2) / 3), 
                            .min.node.size = 30,
                            .splitrule = "variance"
                            )
  
  set.seed(1982)
  
  mod_rf_caret_leafn_species_phylo <- train(
    pp,
    data            = df1h_phylo,
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5
    # num.trees       = 2000,        # boosted for the final model
    # importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafn_species_phylo
  
  saveRDS(mod_rf_caret_leafn_species_phylo, file = filn)
}
```

### Train model based on environment info

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafn_species_env.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafn <- readRDS(filn)
  
} else {
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafN ~ ., data = dplyr::select(df, leafN, all_of(shortlist))) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())


  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = floor((length(shortlist)-1) / 3), 
                            .min.node.size = 30,
                            .splitrule = "variance"
                            )
  
  set.seed(1982)
  
  mod_rf_caret_leafn_species_env <- train(
    pp,
    data            = dplyr::select(df, leafN, all_of(shortlist)),
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5
    # num.trees       = 2000,        # boosted for the final model
    # importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafn_species_env
  
  saveRDS(mod_rf_caret_leafn_species_env, file = filn)
}
```

#### Plot CV results

Visualise cross-validation results using results from the best tuned model.
```{r}
mod_rf_caret_leafn <- readRDS(paste0(here::here(), "/data/mod_rf_caret_leafn.rds"))

## get predicted values from cross-validation resamples, take mean across repetitions
df_cv <- mod_rf_caret_leafn$pred %>% 
  as_tibble() %>% 
  dplyr::filter(mtry == mod_rf_caret_leafn$bestTune$mtry, 
                splitrule == mod_rf_caret_leafn$bestTune$splitrule, 
                min.node.size == mod_rf_caret_leafn$bestTune$min.node.size) %>%
  separate(Resample, into = c(NA, "Fold"), sep = "old") %>% 
  dplyr::rename(idx = rowIndex)

out <- df_cv %>% 
  rbeni::analyse_modobs2("pred", "obs", type = "heat", shortsubtitle = TRUE)
gg1 <- out$gg +
  ylim(5,40) + xlim(5,40) +
  labs(x = "Predicted leaf N (mg/g)", y = "Observed leaf N (mg/g)")
gg1
```


### Example partial dependence analysis

```{r}
library(pdp)
pred <- function(object, newdata)  {
  results <- as.vector(predict(object, newdata))
  return(results)
}

partial(
  mod_rf_caret_leafn,
  train = dplyr::select(dfs, leafN, all_of(shortlist)), 
  pred.var = "ndep",
  pred.fun = pred,
  grid.resolution = 20,
  plot = TRUE,
  center = TRUE,
  plot.engine = "ggplot2"
)
```



<!-- ### Example partial dependence analysis -->

<!-- ```{r} -->
<!-- library(pdp) -->
<!-- pred <- function(object, newdata)  { -->
<!--   results <- as.vector(predict(object, newdata)) -->
<!--   return(results) -->
<!-- } -->
<!-- ``` -->


<!-- #### N deposition -->

<!-- ```{r} -->
<!-- gg_partial_ndep <- partial( -->
<!--   mod_rf_caret_leafn, -->
<!--   train = dplyr::select(dfs, leafN, all_of(shortlist)), -->
<!--   pred.var = "ndep", -->
<!--   pred.fun = pred, -->
<!--   grid.resolution = 5, -->
<!--   plot = TRUE, -->
<!--   center = TRUE, -->
<!--   plot.engine = "ggplot2", -->
<!--   alpha = 0.05 -->
<!-- ) -->

<!-- gg_partial_ndep + -->
<!--   theme_classic() + -->
<!--   labs(x = "N deposition") -->
<!-- ``` -->

<!-- #### Aluminum saturation -->

<!-- ```{r} -->
<!-- gg_partial_alsa <- partial( -->
<!--   mod_rf_caret_leafn, -->
<!--   train = dplyr::select(dfs, leafN, all_of(shortlist)), -->
<!--   pred.var = "ALSA", -->
<!--   pred.fun = pred, -->
<!--   grid.resolution = 5, -->
<!--   plot = TRUE, -->
<!--   center = TRUE, -->
<!--   plot.engine = "ggplot2", -->
<!--   alpha = 0.05 -->
<!-- ) -->

<!-- gg_partial_alsa + -->
<!--   theme_classic() + -->
<!--   labs(x = "ALSA") -->
<!-- ``` -->

<!-- #### VPD -->

<!-- ```{r} -->
<!-- gg_partial_vpd <- partial( -->
<!--   mod_rf_caret_leafn, -->
<!--   train = dplyr::select(dfs, leafN, all_of(shortlist)), -->
<!--   pred.var = "mav", -->
<!--   pred.fun = pred, -->
<!--   grid.resolution = 5, -->
<!--   plot = TRUE, -->
<!--   center = TRUE, -->
<!--   plot.engine = "ggplot2", -->
<!--   alpha = 0.05 -->
<!-- ) -->

<!-- gg_partial_vpd + -->
<!--   theme_classic() + -->
<!--   labs(x = "VPD") -->
<!-- ``` -->

<!-- #### Elevation -->

<!-- ```{r} -->
<!-- gg_partial_elv <- partial( -->
<!--   mod_rf_caret_leafn, -->
<!--   train = dplyr::select(dfs, leafN, all_of(shortlist)), -->
<!--   pred.var = "elv", -->
<!--   pred.fun = pred, -->
<!--   grid.resolution = 5, -->
<!--   plot = TRUE, -->
<!--   center = TRUE, -->
<!--   plot.engine = "ggplot2", -->
<!--   alpha = 0.05 -->
<!-- ) -->

<!-- gg_partial_elv + -->
<!--   theme_classic() + -->
<!--   labs(x = "Elevation (m)") -->
<!-- ``` -->

<!-- #### CO2 -->

<!-- ```{r} -->
<!-- gg_partial_ndep <- partial( -->
<!--   mod_rf_caret_leafn, -->
<!--   train = dplyr::select(dfs, leafN, all_of(shortlist)), -->
<!--   pred.var = "co2", -->
<!--   pred.fun = pred, -->
<!--   grid.resolution = 5, -->
<!--   plot = TRUE, -->
<!--   center = TRUE, -->
<!--   plot.engine = "ggplot2", -->
<!--   alpha = 0.05 -->
<!-- ) -->

<!-- gg_partial_ndep + -->
<!--   theme_classic() + -->
<!--   labs(x = "CO2 (ppm)") -->
<!-- ``` -->


<!-- #### Temperature of coldest month -->

<!-- ```{r} -->
<!-- gg_partial_ndep <- partial( -->
<!--   mod_rf_caret_leafn, -->
<!--   train = dplyr::select(dfs, leafN, all_of(shortlist)), -->
<!--   pred.var = "tmonthmin", -->
<!--   pred.fun = pred, -->
<!--   grid.resolution = 5, -->
<!--   plot = TRUE, -->
<!--   center = TRUE, -->
<!--   plot.engine = "ggplot2", -->
<!--   alpha = 0.05 -->
<!-- ) -->

<!-- gg_partial_ndep + -->
<!--   theme_classic() + -->
<!--   labs(x = "CO2 (ppm)") -->
<!-- ``` -->


<!-- #### Irradiance -->

<!-- ```{r} -->
<!-- gg_partial_ndep <- partial( -->
<!--   mod_rf_caret_leafn, -->
<!--   train = dplyr::select(dfs, leafN, all_of(shortlist)), -->
<!--   pred.var = "mai", -->
<!--   pred.fun = pred, -->
<!--   grid.resolution = 5, -->
<!--   plot = TRUE, -->
<!--   center = TRUE, -->
<!--   plot.engine = "ggplot2", -->
<!--   alpha = 0.05 -->
<!-- ) -->

<!-- gg_partial_ndep + -->
<!--   theme_classic() + -->
<!--   labs(x = "CO2 (ppm)") -->
<!-- ``` -->


# Leaf P

### Get FE results

```{r}
target <- "leafP"
df_fe_summary <- read_csv(paste0(here::here(), "/data/df_fe_summary_", target, ".csv"))
df_fe <- read_csv(paste0(here::here(), "/data/df_fe_", target, ".csv"))
df_vip <- read_csv(paste0(here::here(), "/data/df_vip_", target, ".csv"))
```

```{r}
# use this, df_fe_summary was accidentally overwritten. 
df_fe_summary_reconstr <- df_fe |> 
  group_by(level)|> 
  filter(rsq == max(rsq)) |> 
  ungroup()
df_fe_summary_reconstr$pred[nrow(df_fe_summary_reconstr)] <- "ndep"
df_fe_summary_reconstr$rsq[2:nrow(df_fe_summary_reconstr)] <- df_fe_summary_reconstr$rsq[1:(nrow(df_fe_summary_reconstr)-1)]
df_fe_summary_reconstr$step <- 1:nrow(df_fe_summary_reconstr)

all_equal(df_fe_summary, df_fe_summary_reconstr)

tail(df_fe_summary)
tail(df_fe_summary_reconstr)
```

This shows that there are negligible gains after `"pmonthmin"`. In other words, we might as well build a model with just the following predictors:
```{r}
longlist <- df_fe_summary %>% 
  slice((nrow(.)-11):nrow(.)) %>% 
  pull(pred) %>% 
  as.character()
longlist
saveRDS(longlist, file = paste0(here::here(), "/data/longlist_", target, ".rds"))

shortlist <- df_fe_summary %>% 
  slice((nrow(.)-8):nrow(.)) %>% 
  pull(pred) %>% 
  as.character()
shortlist
saveRDS(shortlist, file = paste0(here::here(), "/data/shortlist_", target, ".rds"))
```

```{r}
df_fe_summary %>% 
  mutate(pred = fct_reorder(pred, step)) %>%
  mutate(highlight = ifelse(pred %in% shortlist, "yes", "no")) %>% 
  ggplot(aes(pred, rsq, fill = highlight)) +
  scale_fill_manual( values = c( "yes"="#29a274ff", "no"="#777055ff" ), guide = FALSE ) +
  geom_bar(stat = "identity") +
  labs(x = "", y = expression(italic(R)^2)) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_flip() +
  theme_classic()

ggsave(paste0(here::here(), "/fig/rsq_stepwise_leafP.pdf"), width = 6, height = 8)

gga2 <- df_fe_summary %>% 
  mutate(pred = fct_reorder(pred, step)) %>%
  mutate(highlight = ifelse(pred %in% shortlist, "yes", "no")) %>% 
  tail(n = 13) %>% 
  ggplot(aes(pred, rsq, fill = highlight)) +
  scale_fill_manual( values = c( "yes"="#29a274ff", "no"="#777055ff" ), guide = FALSE ) +
  geom_bar(stat = "identity") +
  labs(title = "Leaf P", x = "", y = expression(italic(R)^2)) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_flip() +
  theme_classic()

gga2
ggsave(paste0(here::here(), "/fig/rsq_stepwise_leafP_sub.pdf"), width = 6, height = 4)
saveRDS(gga2, file = paste0(here::here(), "/data/gga2.rds"))
```

### Train final model

With just these (`longlist`), fit again a RF model.
```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafp.rds")
overwrite <- TRUE

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafp <- readRDS(filn)
  
} else {
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafP ~ ., data = dplyr::select(dfs, leafP, all_of(longlist))) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice based on leaf N
  tune_grid <- expand.grid( .mtry = 3, 
                            .min.node.size = 8,
                            .splitrule = "variance"
                            )
  
  set.seed(1982)
  
  mod_rf_caret_leafp <- train(
    pp,
    data            = dplyr::select(dfs, leafP, all_of(longlist)),
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = FALSE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafp
  
  saveRDS(mod_rf_caret_leafp, file = filn)
}
```

### Plot CV results

Visualise cross-validation results using results from the best tuned model.
```{r}
mod_rf_caret_leafp <- readRDS(filn)

## get predicted values from cross-validation resamples, take mean across repetitions
df_cv <- mod_rf_caret_leafp$pred %>% 
  as_tibble() %>% 
  dplyr::filter(mtry == mod_rf_caret_leafp$bestTune$mtry, 
                splitrule == mod_rf_caret_leafp$bestTune$splitrule, 
                min.node.size == mod_rf_caret_leafp$bestTune$min.node.size) %>%
  separate(Resample, into = c(NA, "Fold"), sep = "old") %>% 
  dplyr::rename(idx = rowIndex)

out <- df_cv %>% 
  rbeni::analyse_modobs2("pred", "obs", type = "heat", shortsubtitle = TRUE)
gg2 <- out$gg +
  ylim(0, 5) + xlim(0, 5) +
  labs(x = "Predicted leaf P (mg/g)", y = "Observed leaf P (mg/g)")
gg2
write_csv(df_cv, file = paste0(here::here(), "/data/df_cv_leafp.csv"))
```

## Leaf N:P

### Get FE results

```{r}
target <- "LeafNP"
df_fe_summary <- read_csv(paste0(here::here(), "/data/df_fe_summary_", target, ".csv"))
df_fe <- read_csv(paste0(here::here(), "/data/df_fe_", target, ".csv"))
df_vip <- read_csv(paste0(here::here(), "/data/df_vip_", target, ".csv"))
```

```{r}
# use this, df_fe_summary was accidentally overwritten. 
df_fe_summary_reconstr <- df_fe |> 
  group_by(level)|> 
  filter(rsq == max(rsq)) |> 
  ungroup()
df_fe_summary_reconstr$pred[nrow(df_fe_summary_reconstr)] <- "co2"
df_fe_summary_reconstr$rsq[2:nrow(df_fe_summary_reconstr)] <- df_fe_summary_reconstr$rsq[1:(nrow(df_fe_summary_reconstr)-1)]
df_fe_summary_reconstr$step <- 1:nrow(df_fe_summary_reconstr)

all_equal(df_fe_summary, df_fe_summary_reconstr)

tail(df_fe_summary)
tail(df_fe_summary_reconstr)
```

This shows that there are negligible gains after `"cwdx80"`. In other words, we might as well build a model with just the following predictors:
```{r}
longlist <- df_fe_summary %>% 
  slice((nrow(.)-8):nrow(.)) %>% 
  pull(pred) %>% 
  as.character()
longlist
saveRDS(longlist, file = paste0(here::here(), "/data/longlist_", target, ".rds"))

shortlist <- df_fe_summary %>% 
  slice((nrow(.)-8):nrow(.)) %>% 
  pull(pred) %>% 
  as.character()
shortlist
saveRDS(shortlist, file = paste0(here::here(), "/data/shortlist_", target, ".rds"))
```

```{r}
df_fe_summary %>% 
  mutate(pred = fct_reorder(pred, step)) %>%
  mutate(highlight = ifelse(pred %in% shortlist, "yes", "no")) %>% 
  ggplot(aes(pred, rsq, fill = highlight)) +
  scale_fill_manual( values = c( "yes"="#29a274ff", "no"="#777055ff" ), guide = FALSE ) +
  geom_bar(stat = "identity") +
  labs(x = "", y = expression(italic(R)^2)) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_flip() +
  theme_classic()

ggsave(paste0(here::here(), "/fig/rsq_stepwise_leafNP.pdf"), width = 6, height = 8)

gga3 <- df_fe_summary %>% 
  mutate(pred = fct_reorder(pred, step)) %>%
  mutate(highlight = ifelse(pred %in% shortlist, "yes", "no")) %>% 
  tail(n = 13) %>% 
  ggplot(aes(pred, rsq, fill = highlight)) +
  scale_fill_manual( values = c( "yes"="#29a274ff", "no"="#777055ff" ), guide = FALSE ) +
  geom_bar(stat = "identity") +
  labs(title = "Leaf N:P", x = "", y = expression(italic(R)^2)) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_flip() +
  theme_classic()

gga3
ggsave(paste0(here::here(), "/fig/rsq_stepwise_leafNP_sub.pdf"), width = 6, height = 4)
saveRDS(gga3, file = paste0(here::here(), "/data/gga3.rds"))
```


### Train final model

For leaf N:P, remove outlier:
```{r}
dfs_sub <- dfs %>% 
  filter(LeafNP < 70)
```

With just these (`longlist`), fit again a RF model.
```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafnp.rds")
overwrite <- TRUE

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafnp <- readRDS(filn)
  
} else {
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(LeafNP ~ ., data = dplyr::select(dfs_sub, LeafNP, all_of(longlist))) %>%
  
    ## impute by median as part of the recipe
    step_medianimpute(all_predictors())
    # step_impute_median(all_predictors())

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice based on leaf N:P
  tune_grid <- expand.grid( .mtry = 3,
                            .min.node.size = 8,
                            .splitrule = "variance"
                            )
  
  set.seed(1982)
  
  mod_rf_caret_leafnp <- train(
    pp,
    data            = dplyr::select(dfs_sub, LeafNP, all_of(longlist)),
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  saveRDS(mod_rf_caret_leafnp, file = filn)
  mod_rf_caret_leafnp
}
```

### Plot CV results

Visualise cross-validation results using results from the best tuned model.
```{r}
## get predicted values from cross-validation resamples, take mean across repetitions
df_cv <- mod_rf_caret_leafnp$pred %>% 
  as_tibble() %>% 
  dplyr::filter(mtry == mod_rf_caret_leafnp$bestTune$mtry, 
                splitrule == mod_rf_caret_leafnp$bestTune$splitrule, 
                min.node.size == mod_rf_caret_leafnp$bestTune$min.node.size) %>%
  separate(Resample, into = c(NA, "Fold"), sep = "old") %>% 
  dplyr::rename(idx = rowIndex)

out <- df_cv %>% 
  rbeni::analyse_modobs2("pred", "obs", type = "heat", shortsubtitle = TRUE)
gg3 <- out$gg +
  ylim(0, 30) + xlim(0, 30) +
  labs(x = "Predicted leaf N:P", y = "Observed leaf N:P")
gg3
write_csv(df_cv, file = paste0(here::here(), "/data/df_cv_leafnp.csv"))
```

# Publication figures

```{r}
library(cowplot)
gg1 <- readRDS(paste0(here::here(), "/data/gg1.rds"))
plot_grid(gg1, gg2, gg3, labels =  c("a", "b", "c"), ncol = 3)
ggsave(paste0(here::here(), "/fig/modobs_all.pdf"), width = 12, height = 4)

plot_grid(gga1, gga2, gga3, labels =  c("a", "b", "c"), ncol = 3)
ggsave(paste0(here::here(), "/fig/fe_all.pdf"), width = 12, height = 4)
```