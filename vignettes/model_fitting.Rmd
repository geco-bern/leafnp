---
title: "Model fitting"
author: "Beni Stocker"
date: "2022-07-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ranger)
library(caret)
library(visdat)
library(vip)
library(pdp)
library(nnet)
library(recipes)
library(knitr)
library(forcats)
library(cowplot)

library(bestNormalize)
library(metafor)
library(nlme)
# library(lme4)
library(lmerTest)
library(effects)
library(ggeffects)
library(MuMIn) # This is the same as Sardan's 2020,output r^2 of the model
library(glmm.hp) # to get specific r^2
library(performance)

df_model_fitting <- tibble()
overwrite <- FALSE
```

## Full data

### Read data

Read full species-level data
```{r}
df <- read_csv("~/data/LeafNP_tiandi/Global_total_leaf_N_P_Di/leafnp_data_covariates_20210702.csv") %>% 
  mutate(grass = tree_shrub_Herb == "H")  
```

Define predictor sets.
```{r}
trgts <- c("leafN", "leafP", "LeafNP")

## predictors excluding PHO, and TS (too many missing)
preds <- c("elv", "mat", "matgs", "tmonthmin", "tmonthmax", "ndaysgs", "mai", "maigs", "map", "pmonthmin", "mapgs", "mavgs", "mav", "alpha", "vcmax25", "jmax25", "gs_accl", "aet", "ai", "cwdx80", "gti", "ndep", "co2", "T_BULK_DENSITY", "AWC_CLASS", "T_CLAY", "T_SILT", "T_SAND", "T_GRAVEL", "T_PH_H2O", "T_TEB", "T_BS", "T_CEC_SOIL", "T_CEC_CLAY", "T_ECE", "T_ESP", "T_CACO3", "T_OC", "ORGC", "TOTN", "CNrt", "ALSA", "PBR", "TP", "TK")

preds_soil <- c("gti", "ndep", "T_BULK_DENSITY", "AWC_CLASS", "T_CLAY", "T_SILT", "T_SAND", "T_GRAVEL", "T_PH_H2O", "T_TEB", "T_BS", "T_CEC_SOIL", "T_CEC_CLAY", "T_ECE", "T_ESP", "T_CACO3", "T_OC", "ORGC", "TOTN", "CNrt", "ALSA", "PBR", "TP", "TK")
  
preds_climate <- c( "elv", "co2", "mat", "matgs", "tmonthmin", "tmonthmax", "ndaysgs", "mai", "maigs", "map", "pmonthmin", "mapgs", "mavgs", "mav", "alpha", "vcmax25", "jmax25", "gs_accl", "aet", "ai", "cwdx80")

preds_pmodeloutputs <- c("vcmax25", "jmax25", "gs_accl")

preds_pmodelinputs <- c("mat", "matgs", "mai", "maigs", "mav", "mavgs", "elv", "co2")
```

Filter to use only data from species that were recorded in at least five different sites.
```{r}
# yields 398 species
use_species <- df %>% 
  dplyr::select(sitename, Species) %>% 
  distinct() %>% 
  group_by(Species) %>% 
  summarise(n = n()) %>% 
  dplyr::filter(n >= 5) %>% 
  pull(Species)

df <- df %>% 
  filter(Species %in% use_species)
```

Prepare date for linear mixed effects modelling. Do data pre-processing as Di Tian implemented it. Transformations are required, but not for Random Forest modelling. Therefore, define separate data objects for the two model types.

Make factors and numeric. No capping to positive numbers done here (as opposed to Di Tian's).
```{r}
df_lmm <- df %>%
  # drop_na() %>%
  rename( 
    SP = Species, 
    FG = FunGroups, 
    TG = tree_shrub_Herb, 
    FGID = Dc_Db_Ec_Eb_Hf_Hg,
    FA = Family, 
    GE = Genus, 
    ID = id
    ) %>%
  mutate(across(all_of(c("sitename", "FG", "FGID", "TG", "FA", "GE", "SP", "ID")), factor)) %>%
  mutate(across(all_of(preds), as.numeric))
```

Apply Yeo-Johnson transformation to make predictors more normally distributed.
```{r}
predict_yeojohnson <- function(x){
  # x[x < 0] <- 0   # How Di did it
  predict( yeojohnson( x ) )
}

df_lmm <- df_lmm %>%
  mutate(across(all_of(preds), predict_yeojohnson))
```

### Leaf N

```{r}
target <- "leafN"
shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds"))
```

#### LMM

```{r}
mod_lmm_leafn <- lme(

  as.formula(paste("log(", target,") ~ ", paste(shortlist, collapse = " + "))),
  random = ~1|SP,
  data = dplyr::select(df_lmm, leafN, all_of(c(shortlist, "SP"))) %>% drop_na()

  )


# This is new released July 21 2021 with many indicators
tmp <- model_performance(mod_lmm_leafn, metrics = "common", verbose = TRUE)
print(tmp)

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "LMM",
                 pred = "combined",
                 rsq = tmp$R2_conditional,
                 rsq_marg = tmp$R2_marginal,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)

# use the ICC for measuring the proportion of the variance explained by the grouping structure in the population (species -> phylo)
addrow <- tibble(target = target,
                 scale = "full",
                 model = "LMM",
                 pred = "phylo",
                 rsq = tmp$ICC,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

#### Random Forest

##### Full

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafn_species.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafn <- readRDS(filn)
  
} else {
  
  ## one-hot encoding for Species identity (warning: 3700 species)
  df1h <- df %>% 
    recipe(leafN ~ ., 
           data = dplyr::select(df, 
                                leafN, 
                                all_of(c(shortlist, "Species", "Family_New", "Genus")))) %>% 
    step_dummy(Species, one_hot = TRUE) %>% 
    step_dummy(Family_New, one_hot = TRUE) %>% 
    step_dummy(Genus, one_hot = TRUE) %>% 
    prep(training = df, retain = TRUE) %>% 
    juice()
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafN ~ ., data = df1h) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = floor((ncol(df1h)-2) / 3), 
                            .min.node.size = c(5),
                            .splitrule = "variance"
                            )

  #   5            2.967203  0.7714976  1.896609
  #  10            2.986728  0.7687592  1.912957
  #  30            3.073594  0.7560441  1.982790
  #  50            3.145462  0.7450257  2.038922
  # 100            3.283074  0.7229009  2.142982
  # 200            3.468583  0.6912312  2.280178
  # 300            3.583204  0.6706687  2.366706
  
  set.seed(1982)
  
  mod_rf_caret_leafn_species <- train(
    pp,
    data            = df1h,
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafn_species
  
  saveRDS(mod_rf_caret_leafn_species, file = filn)
}

# results:
# RMSE      Rsquared   MAE    
# 3.066831  0.7571712  1.98728

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "RF",
                 pred = "combined",
                 rsq = mod_rf_caret_leafn_species$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafn_species$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

##### Species-only

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafn_species_phylo.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafn <- readRDS(filn)
  
} else {
  
  ## one-hot encoding for Species identity (warning: 3700 species)
  df1h_phylo <- df %>% 
    recipe(leafN ~ ., 
           data = dplyr::select(df, leafN, all_of(c("Species", "Family_New", "Genus")))) %>% 
    step_dummy(Species, one_hot = TRUE) %>% 
    step_dummy(Family_New, one_hot = TRUE) %>% 
    step_dummy(Genus, one_hot = TRUE) %>% 
    prep(training = df, retain = TRUE) %>% 
    juice()
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafN ~ ., data = df1h_phylo)

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice (.min.node.size = 30 -> R2 = 0.6402244)
  tune_grid <- expand.grid( .mtry = floor((ncol(df1h_phylo)-2) / 3), 
                            .min.node.size = c(30),
                            .splitrule = "variance"
                            )
  
  set.seed(1982)
  
  mod_rf_caret_leafn_species_phylo <- train(
    pp,
    data            = df1h_phylo,
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafn_species_phylo
  
  saveRDS(mod_rf_caret_leafn_species_phylo, file = filn)
}

  # RMSE      Rsquared   MAE     
  # 3.720109  0.6400929  2.569527

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "RF",
                 pred = "phylo",
                 rsq = mod_rf_caret_leafn_species_phylo$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafn_species_phylo$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

### Leaf P

```{r}
target <- "leafP"
shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds"))
```


#### LMM

```{r}
mod_lmm_leafn <- lme(

  as.formula(paste("log(", target,") ~ ", paste(shortlist, collapse = " + "))),
  random = ~1|SP,
  data = dplyr::select(df_lmm, leafP, all_of(c(shortlist, "SP"))) %>% 
    drop_na()

  )

# This is new released July 21 2021 with many indicators
tmp <- model_performance(mod_lmm_leafn, metrics = "common", verbose = TRUE)
print(tmp)

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "LMM",
                 pred = "combined",
                 rsq = tmp$R2_conditional,
                 rsq_marg = tmp$R2_marginal,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)

# use the ICC for measuring the proportion of the variance explained by the grouping structure in the population (species -> phylo)
addrow <- tibble(target = target,
                 scale = "full",
                 model = "LMM",
                 pred = "phylo",
                 rsq = tmp$ICC,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

#### Random Forest

##### Full

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafp_species.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafp <- readRDS(filn)
  
} else {
  
  ## one-hot encoding for Species identity (warning: 3700 species)
  df1h <- df %>% 
    recipe(leafP ~ ., 
           data = dplyr::select(df, 
                                leafP, 
                                all_of(c(shortlist, "Species", "Family_New", "Genus")))) %>% 
    step_dummy(Species, one_hot = TRUE) %>% 
    step_dummy(Family_New, one_hot = TRUE) %>% 
    step_dummy(Genus, one_hot = TRUE) %>% 
    prep(training = df, retain = TRUE) %>% 
    juice()
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafP ~ ., data = df1h) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())


  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = floor((ncol(df1h)-2) / 3),
                            .min.node.size = 5,
                            .splitrule = "variance"
                            )

#   mtry  min.node.size  RMSE       Rsquared   MAE      
#   100    5             0.3874082  0.5363492  0.2447423
#   100   10             0.3904482  0.5303209  0.2472214
#   100   30             0.4004939  0.5093304  0.2552350
#   150    5             0.3805906  0.5485776  0.2366451
#   150   10             0.3833998  0.5431304  0.2388715
#   150   30             0.3938485  0.5216406  0.2474056
#   200    5             0.3787757  0.5508954  0.2341348
#   200   10             0.3812722  0.5459759  0.2360283
#   200   30             0.3913629  0.5248930  0.2441936
#   250    5             0.3780353  0.5518580  0.2335030
#   250   10             0.3801319  0.5477035  0.2349782
#   250   30             0.3894464  0.5283005  0.2426346
  # 250    5             0.3781431  0.5515713  0.2336278
  # 250   10             0.3805369  0.5466214  0.2352856
  # 500    5             0.3785359  0.5497682  0.2339180
  # 500   10             0.3804690  0.5455741  0.2351515
# 
# Tuning parameter 'splitrule' was held constant at a value of variance
# RMSE was used to select the optimal model using the smallest value.
# The final values used for the model were mtry = 250, splitrule = variance and min.node.size = 5.  
    
  set.seed(1982)
  
  mod_rf_caret_leafp_species <- train(
    pp,
    data            = df1h,
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafp_species
  
  saveRDS(mod_rf_caret_leafp_species, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "RF",
                 pred = "combined",
                 rsq = mod_rf_caret_leafp_species$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafp_species$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

##### Species-only

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafp_species_phylo.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafp <- readRDS(filn)
  
} else {
  
  ## one-hot encoding for Species identity (warning: 3700 species)
  df1h_phylo <- df %>% 
    recipe(leafP ~ ., data = dplyr::select(df, leafP, all_of(c("Species", "Family_New", "Genus")))) %>% 
    step_dummy(Species, one_hot = TRUE) %>% 
    step_dummy(Family_New, one_hot = TRUE) %>% 
    step_dummy(Genus, one_hot = TRUE) %>% 
    prep(training = df, retain = TRUE) %>% 
    juice()
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafP ~ ., data = df1h_phylo)

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = floor((ncol(df1h_phylo)-2) / 3), 
                            .min.node.size = 30,    # no gain in lower values, tried c(5, 10, 30)
                            .splitrule = "variance"
                            )
  
  set.seed(1982)
  
  mod_rf_caret_leafp_species_phylo <- train(
    pp,
    data            = df1h_phylo,
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafp_species_phylo
  
  saveRDS(mod_rf_caret_leafp_species_phylo, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "RF",
                 pred = "phylo",
                 rsq = mod_rf_caret_leafp_species_phylo$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafp_species_phylo$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

### Leaf N:P

```{r}
target <- "LeafNP"
shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds"))
```

#### LMM

```{r}
mod_lmm_leafn <- lme(

  as.formula(paste("log(", target,") ~ ", paste(shortlist, collapse = " + "))),
  random = ~1|SP,
  data = dplyr::select(df_lmm, LeafNP, all_of(c(shortlist, "SP"))) %>% drop_na()

  )


# This is new released July 21 2021 with many indicators
tmp <- model_performance(mod_lmm_leafn, metrics = "common", verbose = TRUE)
print(tmp)

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "LMM",
                 pred = "combined",
                 rsq = tmp$R2_conditional,
                 rsq_marg = tmp$R2_marginal,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)

# use the ICC for measuring the proportion of the variance explained by the grouping structure in the population (species -> phylo)
addrow <- tibble(target = target,
                 scale = "full",
                 model = "LMM",
                 pred = "phylo",
                 rsq = tmp$ICC,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

#### Random Forest

##### Full

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafnp_species.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafnp <- readRDS(filn)
  
} else {
  
  ## one-hot encoding for Species identity (warning: 3700 species)
  df1h <- df %>% 
    recipe(LeafNP ~ ., data = dplyr::select(df, LeafNP, all_of(c(shortlist, "Species", "Family_New", "Genus")))) %>% 
    step_dummy(Species, one_hot = TRUE) %>% 
    step_dummy(Family_New, one_hot = TRUE) %>% 
    step_dummy(Genus, one_hot = TRUE) %>% 
    prep(training = df, retain = TRUE) %>% 
    juice()
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(LeafNP ~ ., data = df1h) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())


  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = floor((ncol(df1h)-2) / 3), 
                            .min.node.size = 10,  # tried, c(5, 10, 30, 40),
                            .splitrule = "variance"
                            )
  
  set.seed(1982)
  
  mod_rf_caret_leafnp_species <- train(
    pp,
    data            = df1h,
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafnp_species
  
  saveRDS(mod_rf_caret_leafnp_species, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "RF",
                 pred = "combined",
                 rsq = mod_rf_caret_leafnp_species$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafnp_species$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

##### Species-only

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafnp_species_phylo.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafnp <- readRDS(filn)
  
} else {
  
  ## one-hot encoding for Species identity (warning: 3700 species)
  df1h_phylo <- df %>% 
    recipe(LeafNP ~ ., data = dplyr::select(df, LeafNP, all_of(c("Species", "Family_New", "Genus")))) %>% 
    step_dummy(Species, one_hot = TRUE) %>% 
    step_dummy(Family_New, one_hot = TRUE) %>% 
    step_dummy(Genus, one_hot = TRUE) %>% 
    prep(training = df, retain = TRUE) %>% 
    juice()
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(LeafNP ~ ., data = df1h_phylo)

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = floor((ncol(df1h)-2) / 3), 
                            .min.node.size = 10,
                            .splitrule = "variance"
                            )
  # min.node.size  RMSE      Rsquared   MAE     
  #  5             6.306300  0.3440797  3.156061
  # 10             6.301759  0.3450017  3.155171
  # 30             6.303985  0.3443676  3.156464
    
  set.seed(1982)
  
  mod_rf_caret_leafnp_species_phylo <- train(
    pp,
    data            = df1h_phylo,
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafnp_species_phylo
  
  saveRDS(mod_rf_caret_leafnp_species_phylo, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "RF",
                 pred = "phylo",
                 rsq = mod_rf_caret_leafnp_species_phylo$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafnp_species_phylo$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

## Aggregated to sites

### Aggregate data

Load data aggregated to sites, done in `randomforest_leafnp.Rmd`.
```{r}
filn <- paste0(here::here(), "/data/dfs_leafnp_20210729.rds")

if (!file.exists(filn)){
  
  dfs <- df %>%

    ## remove unplausible leafNP data baed on Dis recommendation
    dplyr::filter(LeafNP < 70) %>%
  
    mutate(elv_grp = elv) %>%
    group_by(lon, lat, elv_grp, sitename) %>%
    summarise(across(c(preds, trgts), ~(mean(.x, na.rm = TRUE)))) %>%
    left_join(df %>%
                group_by(sitename) %>%
                summarise(nobs = n()),
              by = "sitename") %>%
    ungroup()
  
  saveRDS(dfs, file = filn)
  
} else {
  dfs <- readRDS(filn) 
}
```


### Leaf N

```{r}
target <- "leafN"
shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds"))
```

#### LM

```{r}
mod_lm_leafn <- lm(

  as.formula(paste("log(", target,") ~ ", paste(shortlist, collapse = " + "))),
  data = dplyr::select(df_lmm, leafN, all_of(shortlist)) %>% drop_na()

  )


# This is new released July 21 2021 with many indicators
tmp <- model_performance(mod_lm_leafn, metrics = "common", verbose = TRUE)
print(tmp)

# collect results into df
addrow <- tibble(target = target,
                 scale = "aggregated",
                 model = "LM",
                 pred = "combined",
                 rsq = tmp$R2,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

#### Random Forest

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafn.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafn <- readRDS(filn)
  
} else {
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafN ~ ., data = dplyr::select(dfs, leafN, all_of(shortlist))) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = 3,
                            .min.node.size = 5,
                            .splitrule = "variance"
                            )
  
#   mtry  min.node.size  RMSE      Rsquared   MAE     
#   2      5             4.884196  0.4479481  3.514411
#   2      8             4.896592  0.4455124  3.529126
#   2     10             4.919218  0.4405717  3.544740
#   2     30             5.007515  0.4218591  3.611234
#   3      5             4.870319  0.4503581  3.500935
#   3      8             4.881588  0.4481474  3.511033
#   3     10             4.890466  0.4463209  3.519473
#   3     30             4.988757  0.4248672  3.591211
#   4      5             4.871780  0.4497299  3.503742
#   4      8             4.882993  0.4472821  3.509671
#   4     10             4.893394  0.4451099  3.518867
#   4     30             4.968471  0.4289072  3.572633
#   5      5             4.878588  0.4480658  3.503187
#   5      8             4.879484  0.4479626  3.504207
#   5     10             4.894362  0.4447319  3.517260
#   5     30             4.968262  0.4284104  3.570183
# 
# Tuning parameter 'splitrule' was held constant at a value of variance
# RMSE was used to select the optimal model using the smallest value.
# The final values used for the model were mtry = 3, splitrule = variance and min.node.size = 5.
  
  set.seed(1982)
  
  mod_rf_caret_leafn <- train(
    pp,
    data            = dplyr::select(dfs, leafN, all_of(shortlist)),
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafn
  
  saveRDS(mod_rf_caret_leafn, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "aggregated",
                 model = "RF",
                 pred = "combined",
                 rsq = mod_rf_caret_leafn$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafn$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

### Leaf P

```{r}
target <- "leafP"
shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds"))
```

#### LM

```{r}
mod_lm_leafn <- lm(

  as.formula(paste("log(", target,") ~ ", paste(shortlist, collapse = " + "))),
  data = dplyr::select(df_lmm, leafP, all_of(shortlist)) %>% drop_na()

  )


# This is new released July 21 2021 with many indicators
tmp <- model_performance(mod_lm_leafn, metrics = "common", verbose = TRUE)
print(tmp)

# collect results into df
addrow <- tibble(target = target,
                 scale = "aggregated",
                 model = "LM",
                 pred = "combined",
                 rsq = tmp$R2,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

#### Random Forest

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafp.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafp <- readRDS(filn)
  
} else {
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafP ~ ., data = dplyr::select(dfs, leafP, all_of(shortlist))) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = 3, 
                            .min.node.size = 5,
                            .splitrule = "variance"
                            )

#   mtry  min.node.size  RMSE       Rsquared   MAE      
#   2      5             0.5042681  0.3357895  0.3243617
#   2      8             0.5051088  0.3342590  0.3247737
#   2     10             0.5056553  0.3332677  0.3251537
#   2     30             0.5128529  0.3166988  0.3304216
#   3      5             0.5034861  0.3373895  0.3240154
#   3      8             0.5042383  0.3358247  0.3246955
#   3     10             0.5046995  0.3346839  0.3249082
#   3     30             0.5116329  0.3181036  0.3294687
#   4      5             0.5044254  0.3346370  0.3250521
#   4      8             0.5042814  0.3352189  0.3245283
#   4     10             0.5051817  0.3329689  0.3252385
#   4     30             0.5107109  0.3195499  0.3286733
#   5      5             0.5043169  0.3348144  0.3243951
#   5      8             0.5052328  0.3324299  0.3257510
#   5     10             0.5055748  0.3317521  0.3256664
#   5     30             0.5106812  0.3189370  0.3288055
# 
# Tuning parameter 'splitrule' was held constant at a value of variance
# RMSE was used to select the optimal model using the smallest value.
# The final values used for the model were mtry = 3, splitrule = variance and min.node.size = 5.  
  
  set.seed(1982)
  
  mod_rf_caret_leafp <- train(
    pp,
    data            = dplyr::select(dfs, leafP, all_of(shortlist)),
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafp
  
  saveRDS(mod_rf_caret_leafp, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "aggregated",
                 model = "RF",
                 pred = "combined",
                 rsq = mod_rf_caret_leafp$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafp$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

### Leaf N:P

```{r}
target <- "LeafNP"
shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds"))
```

#### LM

```{r}
mod_lm_leafn <- lm(

  as.formula(paste("log(", target,") ~ ", paste(shortlist, collapse = " + "))),
  data = dplyr::select(df_lmm, LeafNP, all_of(shortlist)) %>% drop_na()

  )


# This is new released July 21 2021 with many indicators
tmp <- model_performance(mod_lm_leafn, metrics = "common", verbose = TRUE)
print(tmp)

# collect results into df
addrow <- tibble(target = target,
                 scale = "aggregated",
                 model = "LM",
                 pred = "combined",
                 rsq = tmp$R2,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

#### Random Forest

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafnp.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafnp <- readRDS(filn)
  
} else {
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(LeafNP ~ ., data = dplyr::select(dfs, LeafNP, all_of(shortlist))) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = 2, 
                            .min.node.size = 5,
                            .splitrule = "variance"
                            )
  
#   mtry  min.node.size  RMSE      Rsquared   MAE     
#   2      5             5.528117  0.2790776  3.442130
#   2      8             5.533388  0.2778906  3.442768
#   2     10             5.542267  0.2754941  3.451749
#   2     30             5.590654  0.2642769  3.489848
#   3      5             5.534519  0.2772633  3.445870
#   3      8             5.536107  0.2769081  3.450359
#   3     10             5.535879  0.2770679  3.451145
#   3     30             5.576416  0.2670492  3.474853
#   4      5             5.534420  0.2775807  3.449510
#   4      8             5.540016  0.2758562  3.451583
#   4     10             5.540370  0.2756434  3.450493
#   4     30             5.574360  0.2671489  3.473193
#   5      5             5.545552  0.2748855  3.458211
#   5      8             5.549643  0.2734559  3.459849
#   5     10             5.547744  0.2738793  3.460229
#   5     30             5.577375  0.2661357  3.476393
# 
# Tuning parameter 'splitrule' was held constant at a value of variance
# RMSE was used to select the optimal model using the smallest value.
# The final values used for the model were mtry = 2, splitrule = variance and min.node.size = 5.
    
  set.seed(1982)
  
  mod_rf_caret_leafnp <- train(
    pp,
    data            = dplyr::select(dfs, LeafNP, all_of(shortlist)),
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafnp
  
  saveRDS(mod_rf_caret_leafnp, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "aggregated",
                 model = "RF",
                 pred = "combined",
                 rsq = mod_rf_caret_leafnp$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafnp$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

## Visualisation

Explanation:

For the data aggregated to site-level (a, c, d), the R2 represents the coefficient of determination on a multivariate linear regression model (LM), and the mean across a 5-fold cross validation for the RF. Only environmental factors are considered as model predictors. 

For the non-aggregated, species-level data, The top edge of the bar represents the conditional R-squared for the linear mixed effects models (LMM) and the mean across a 5-fold cross validation for the RF, using environmental and phylogenetic variables as predictors. Species identity is considered as the grouping variable for random intercepts in the LMM and is considered as a one-hot encoded predictor in the RF. The "phylogenetic" portion (brown bar) visualises the the intraclass correlation coefficient (ICC) for the LMMs, determined on the same model and representing the proportion of the variance explained by species identity. For the RF models, the "phylogenetic" pportion of the bar represents the R-quared (mean across 5-fold cross validation) of a model that contains only species, genus, and family identity (one-hot encoded) as predictors.





```{r}
# N
gg1 <- df_model_fitting %>% 
  filter(target == "leafN" & scale == "aggregated") %>% 
  ggplot(aes(x = model, y = rsq)) +
  geom_bar(stat="identity", position = "identity", fill = "#29a274ff") +
  theme_classic() +
  labs(title ="Leaf N", subtitle = "Site-level data", y = expression(italic(R)^2), x = "") +
  scale_y_continuous(limits = c(0, 0.8), expand = c(0, 0))

gg2 <- df_model_fitting %>% 
  filter(target == "leafN" & scale == "full") %>% 
  ggplot(aes(x = model, y = rsq, fill = pred)) +
  geom_bar(stat="identity", position = "identity") +
  theme_classic() +
  labs(title ="Leaf N", subtitle = "Species-level data", y = expression(italic(R)^2), x = "") +
  scale_fill_manual(values = c("#29a274ff", "#777055ff"), labels = c("Environment", "Phylogenetic"), name = NULL) +
  scale_y_continuous(limits = c(0, 0.8), expand = c(0, 0))

# P
gg3 <- df_model_fitting %>% 
  filter(target == "leafP" & scale == "aggregated") %>% 
  ggplot(aes(x = model, y = rsq)) +
  geom_bar(stat="identity", position = "identity", fill = "#29a274ff") +
  theme_classic() +
  labs(title ="Leaf P", subtitle = "Site-level data", y = expression(italic(R)^2), x = "") +
  scale_y_continuous(limits = c(0, 0.8), expand = c(0, 0))

gg4 <- df_model_fitting %>% 
  filter(target == "leafP" & scale == "full") %>% 
  ggplot(aes(x = model, y = rsq, fill = pred)) +
  geom_bar(stat="identity", position = "identity") +
  theme_classic() +
  labs(title ="Leaf P", subtitle = "Species-level data", y = expression(italic(R)^2), x = "") +
  scale_fill_manual(values = c("#29a274ff", "#777055ff"), labels = c("Environment", "Phylogenetic"), name = NULL) +
  scale_y_continuous(limits = c(0, 0.8), expand = c(0, 0))

# N:P
gg5 <- df_model_fitting %>% 
  filter(target == "LeafNP" & scale == "aggregated") %>% 
  ggplot(aes(x = model, y = rsq)) +
  geom_bar(stat="identity", position = "identity", fill = "#29a274ff") +
  theme_classic() +
  labs(title ="Leaf N:P", subtitle = "Site-level data", y = expression(italic(R)^2), x = "") +
  scale_y_continuous(limits = c(0, 0.8), expand = c(0, 0))

gg6 <- df_model_fitting %>% 
  filter(target == "LeafNP" & scale == "full") %>% 
  ggplot(aes(x = model, y = rsq, fill = pred)) +
  geom_bar(stat="identity", position = "identity") +
  theme_classic() +
  labs(title ="Leaf N:P", subtitle = "Species-level data", y = expression(italic(R)^2), x = "") +
  scale_fill_manual(values = c("#29a274ff", "#777055ff"), labels = c("Environment", "Phylogenetic"), name = NULL) +
  scale_y_continuous(limits = c(0, 0.8), expand = c(0, 0))

plot_grid(gg1, gg2, rel_widths = c(0.5, 0.8), ncol = 2)
plot_grid(gg3, gg4, rel_widths = c(0.5, 0.8), ncol = 2)
plot_grid(gg5, gg6, rel_widths = c(0.5, 0.8), ncol = 2)

plot_grid(gg1, gg2, gg3, gg4, gg5, gg6, rel_widths = c(0.5, 0.8), ncol = 2, labels = c("a", "b", "c", "d", "e", "f"))
ggsave(paste0(here::here(), "/fig/bars_model_fitting.pdf"), width = 6, height = 8)
```
