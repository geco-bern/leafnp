---
title: "Model fitting"
author: "Beni Stocker"
date: "2022-07-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ranger)
library(caret)
library(visdat)
library(vip)
library(pdp)
library(nnet)
library(recipes)
library(knitr)
library(forcats)
library(cowplot)

library(bestNormalize)
library(metafor)
library(nlme)
# library(lme4)
library(lmerTest)
library(effects)
library(ggeffects)
library(MuMIn) # This is the same as Sardan's 2020,output r^2 of the model
library(glmm.hp) # to get specific r^2
library(performance)

df_model_fitting <- tibble()
overwrite <- FALSE
```

## Full data

### Read data

Read full species-level data
```{r}
df <- read_csv("~/data/LeafNP_tiandi/Global_total_leaf_N_P_Di/leafnp_data_covariates_20210702.csv") %>% 
  mutate(grass = tree_shrub_Herb == "H")  
```

Define predictor sets.
```{r}
trgts <- c("leafN", "leafP", "LeafNP")

## predictors excluding PHO, and TS (too many missing)
preds <- c("elv", "mat", "matgs", "tmonthmin", "tmonthmax", "ndaysgs", "mai", "maigs", "map", "pmonthmin", "mapgs", "mavgs", "mav", "alpha", "vcmax25", "jmax25", "gs_accl", "aet", "ai", "cwdx80", "gti", "ndep", "co2", "T_BULK_DENSITY", "AWC_CLASS", "T_CLAY", "T_SILT", "T_SAND", "T_GRAVEL", "T_PH_H2O", "T_TEB", "T_BS", "T_CEC_SOIL", "T_CEC_CLAY", "T_ECE", "T_ESP", "T_CACO3", "T_OC", "ORGC", "TOTN", "CNrt", "ALSA", "PBR", "TP", "TK")

preds_soil <- c("gti", "ndep", "T_BULK_DENSITY", "AWC_CLASS", "T_CLAY", "T_SILT", "T_SAND", "T_GRAVEL", "T_PH_H2O", "T_TEB", "T_BS", "T_CEC_SOIL", "T_CEC_CLAY", "T_ECE", "T_ESP", "T_CACO3", "T_OC", "ORGC", "TOTN", "CNrt", "ALSA", "PBR", "TP", "TK")
  
preds_climate <- c( "elv", "co2", "mat", "matgs", "tmonthmin", "tmonthmax", "ndaysgs", "mai", "maigs", "map", "pmonthmin", "mapgs", "mavgs", "mav", "alpha", "vcmax25", "jmax25", "gs_accl", "aet", "ai", "cwdx80")

preds_pmodeloutputs <- c("vcmax25", "jmax25", "gs_accl")

preds_pmodelinputs <- c("mat", "matgs", "mai", "maigs", "mav", "mavgs", "elv", "co2")
```

Filter to use only data from species that were recorded in at least five different sites.
```{r}
# yields 398 species
use_species <- df %>% 
  dplyr::select(sitename, Species) %>% 
  distinct() %>% 
  group_by(Species) %>% 
  summarise(n = n()) %>% 
  dplyr::filter(n >= 5) %>% 
  pull(Species)

df <- df %>% 
  filter(Species %in% use_species)
```

Prepare date for linear mixed effects modelling. Do data pre-processing as Di Tian implemented it. Transformations are required, but not for Random Forest modelling. Therefore, define separate data objects for the two model types.

Make factors and numeric. No capping to positive numbers done here (as opposed to Di Tian's).
```{r}
df_lmm <- df %>%
  # drop_na() %>%
  rename( 
    SP = Species, 
    FG = FunGroups, 
    TG = tree_shrub_Herb, 
    FGID = Dc_Db_Ec_Eb_Hf_Hg,
    FA = Family, 
    GE = Genus, 
    ID = id
    ) %>%
  mutate(across(all_of(c("sitename", "FG", "FGID", "TG", "FA", "GE", "SP", "ID")), factor)) %>%
  mutate(across(all_of(preds), as.numeric))
```

Apply Yeo-Johnson transformation to make predictors more normally distributed.
```{r}
predict_yeojohnson <- function(x){
  # x[x < 0] <- 0   # How Di did it
  predict( yeojohnson( x ) )
}

df_lmm <- df_lmm %>%
  mutate(across(all_of(preds), predict_yeojohnson))
```

### Leaf N

```{r}
target <- "leafN"
shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds"))
```

#### LMM

```{r}
mod_lmm_leafn <- lme(

  as.formula(paste("log(", target,") ~ ", paste(shortlist, collapse = " + "))),
  random = ~1|SP,
  data = dplyr::select(df_lmm, leafN, all_of(c(shortlist, "SP"))) %>% drop_na()

  )


# This is new released July 21 2021 with many indicators
tmp <- model_performance(mod_lmm_leafn, metrics = "common", verbose = TRUE)
print(tmp)

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "LMM",
                 pred = "combined",
                 rsq = tmp$R2_conditional,
                 rsq_marg = tmp$R2_marginal,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)

# use the ICC for measuring the proportion of the variance explained by the grouping structure in the population (species -> phylo)
addrow <- tibble(target = target,
                 scale = "full",
                 model = "LMM",
                 pred = "phylo",
                 rsq = tmp$ICC,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

#### Random Forest

##### Full

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafn_species.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafn_species <- readRDS(filn)
  
} else {
  
  ## one-hot encoding for Species identity (warning: 3700 species)
  df1h <- df %>% 
    recipe(leafN ~ ., 
           data = dplyr::select(df, 
                                leafN, 
                                all_of(c(shortlist, "Species", "Family_New", "Genus")))) %>% 
    step_dummy(Species, one_hot = TRUE) %>% 
    step_dummy(Family_New, one_hot = TRUE) %>% 
    step_dummy(Genus, one_hot = TRUE) %>% 
    prep(training = df, retain = TRUE) %>% 
    juice()
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafN ~ ., data = df1h) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = floor((ncol(df1h)-2) / 3), 
                            .min.node.size = c(5),
                            .splitrule = "variance"
                            )

  #   5            2.967203  0.7714976  1.896609
  #  10            2.986728  0.7687592  1.912957
  #  30            3.073594  0.7560441  1.982790
  #  50            3.145462  0.7450257  2.038922
  # 100            3.283074  0.7229009  2.142982
  # 200            3.468583  0.6912312  2.280178
  # 300            3.583204  0.6706687  2.366706
  
  set.seed(1982)
  
  mod_rf_caret_leafn_species <- train(
    pp,
    data            = df1h,
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafn_species
  
  saveRDS(mod_rf_caret_leafn_species, file = filn)
}

# results:
# RMSE      Rsquared   MAE    
# 3.066831  0.7571712  1.98728

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "RF",
                 pred = "combined",
                 rsq = mod_rf_caret_leafn_species$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafn_species$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

##### Species-only

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafn_species_phylo.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafn_species_phylo <- readRDS(filn)
  
} else {
  
  ## one-hot encoding for Species identity (warning: 3700 species)
  df1h_phylo <- df %>% 
    recipe(leafN ~ ., 
           data = dplyr::select(df, leafN, all_of(c("Species", "Family_New", "Genus")))) %>% 
    step_dummy(Species, one_hot = TRUE) %>% 
    step_dummy(Family_New, one_hot = TRUE) %>% 
    step_dummy(Genus, one_hot = TRUE) %>% 
    prep(training = df, retain = TRUE) %>% 
    juice()
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafN ~ ., data = df1h_phylo)

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice (.min.node.size = 30 -> R2 = 0.6402244)
  tune_grid <- expand.grid( .mtry = floor((ncol(df1h_phylo)-2) / 3), 
                            .min.node.size = c(30),
                            .splitrule = "variance"
                            )
  
  set.seed(1982)
  
  mod_rf_caret_leafn_species_phylo <- train(
    pp,
    data            = df1h_phylo,
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafn_species_phylo
  
  saveRDS(mod_rf_caret_leafn_species_phylo, file = filn)
}

  # RMSE      Rsquared   MAE     
  # 3.720109  0.6400929  2.569527

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "RF",
                 pred = "phylo",
                 rsq = mod_rf_caret_leafn_species_phylo$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafn_species_phylo$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

### Leaf P

```{r}
target <- "leafP"
shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds"))
```


#### LMM

```{r}
mod_lmm_leafp <- lme(

  as.formula(paste("log(", target,") ~ ", paste(shortlist, collapse = " + "))),
  random = ~1|SP,
  data = dplyr::select(df_lmm, leafP, all_of(c(shortlist, "SP"))) %>% 
    drop_na()

  )

# This is new released July 21 2021 with many indicators
tmp <- model_performance(mod_lmm_leafp, metrics = "common", verbose = TRUE)
print(tmp)

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "LMM",
                 pred = "combined",
                 rsq = tmp$R2_conditional,
                 rsq_marg = tmp$R2_marginal,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)

# use the ICC for measuring the proportion of the variance explained by the grouping structure in the population (species -> phylo)
addrow <- tibble(target = target,
                 scale = "full",
                 model = "LMM",
                 pred = "phylo",
                 rsq = tmp$ICC,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

#### Random Forest

##### Full

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafp_species.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafp_species <- readRDS(filn)
  
} else {
  
  ## one-hot encoding for Species identity (warning: 3700 species)
  df1h <- df %>% 
    recipe(leafP ~ ., 
           data = dplyr::select(df, 
                                leafP, 
                                all_of(c(shortlist, "Species", "Family_New", "Genus")))) %>% 
    step_dummy(Species, one_hot = TRUE) %>% 
    step_dummy(Family_New, one_hot = TRUE) %>% 
    step_dummy(Genus, one_hot = TRUE) %>% 
    prep(training = df, retain = TRUE) %>% 
    juice()
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafP ~ ., data = df1h) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())


  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = floor((ncol(df1h)-2) / 3),
                            .min.node.size = 5,
                            .splitrule = "variance"
                            )

#   mtry  min.node.size  RMSE       Rsquared   MAE      
#   100    5             0.3874082  0.5363492  0.2447423
#   100   10             0.3904482  0.5303209  0.2472214
#   100   30             0.4004939  0.5093304  0.2552350
#   150    5             0.3805906  0.5485776  0.2366451
#   150   10             0.3833998  0.5431304  0.2388715
#   150   30             0.3938485  0.5216406  0.2474056
#   200    5             0.3787757  0.5508954  0.2341348
#   200   10             0.3812722  0.5459759  0.2360283
#   200   30             0.3913629  0.5248930  0.2441936
#   250    5             0.3780353  0.5518580  0.2335030
#   250   10             0.3801319  0.5477035  0.2349782
#   250   30             0.3894464  0.5283005  0.2426346
  # 250    5             0.3781431  0.5515713  0.2336278
  # 250   10             0.3805369  0.5466214  0.2352856
  # 500    5             0.3785359  0.5497682  0.2339180
  # 500   10             0.3804690  0.5455741  0.2351515
# 
# Tuning parameter 'splitrule' was held constant at a value of variance
# RMSE was used to select the optimal model using the smallest value.
# The final values used for the model were mtry = 250, splitrule = variance and min.node.size = 5.  
    
  set.seed(1982)
  
  mod_rf_caret_leafp_species <- train(
    pp,
    data            = df1h,
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafp_species
  
  saveRDS(mod_rf_caret_leafp_species, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "RF",
                 pred = "combined",
                 rsq = mod_rf_caret_leafp_species$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafp_species$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

##### Species-only

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafp_species_phylo.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafp_species_phylo <- readRDS(filn)
  
} else {
  
  ## one-hot encoding for Species identity (warning: 3700 species)
  df1h_phylo <- df %>% 
    recipe(leafP ~ ., data = dplyr::select(df, leafP, all_of(c("Species", "Family_New", "Genus")))) %>% 
    step_dummy(Species, one_hot = TRUE) %>% 
    step_dummy(Family_New, one_hot = TRUE) %>% 
    step_dummy(Genus, one_hot = TRUE) %>% 
    prep(training = df, retain = TRUE) %>% 
    juice()
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafP ~ ., data = df1h_phylo)

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = floor((ncol(df1h_phylo)-2) / 3), 
                            .min.node.size = 30,    # no gain in lower values, tried c(5, 10, 30)
                            .splitrule = "variance"
                            )
  
  set.seed(1982)
  
  mod_rf_caret_leafp_species_phylo <- train(
    pp,
    data            = df1h_phylo,
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafp_species_phylo
  
  saveRDS(mod_rf_caret_leafp_species_phylo, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "RF",
                 pred = "phylo",
                 rsq = mod_rf_caret_leafp_species_phylo$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafp_species_phylo$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

### Leaf N:P

```{r}
target <- "LeafNP"
shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds"))
```

#### LMM

```{r}
mod_lmm_leafnp <- lme(

  as.formula(paste("log(", target,") ~ ", paste(shortlist, collapse = " + "))),
  random = ~1|SP,
  data = dplyr::select(df_lmm, LeafNP, all_of(c(shortlist, "SP"))) %>% drop_na()

  )


# This is new released July 21 2021 with many indicators
tmp <- model_performance(mod_lmm_leafnp, metrics = "common", verbose = TRUE)
print(tmp)

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "LMM",
                 pred = "combined",
                 rsq = tmp$R2_conditional,
                 rsq_marg = tmp$R2_marginal,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)

# use the ICC for measuring the proportion of the variance explained by the grouping structure in the population (species -> phylo)
addrow <- tibble(target = target,
                 scale = "full",
                 model = "LMM",
                 pred = "phylo",
                 rsq = tmp$ICC,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

#### Random Forest

##### Full

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafnp_species.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafnp_species <- readRDS(filn)
  
} else {
  
  ## one-hot encoding for Species identity (warning: 3700 species)
  df1h <- df %>% 
    recipe(LeafNP ~ ., data = dplyr::select(df, LeafNP, all_of(c(shortlist, "Species", "Family_New", "Genus")))) %>% 
    step_dummy(Species, one_hot = TRUE) %>% 
    step_dummy(Family_New, one_hot = TRUE) %>% 
    step_dummy(Genus, one_hot = TRUE) %>% 
    prep(training = df, retain = TRUE) %>% 
    juice()
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(LeafNP ~ ., data = df1h) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())


  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = floor((ncol(df1h)-2) / 3), 
                            .min.node.size = 10,  # tried, c(5, 10, 30, 40),
                            .splitrule = "variance"
                            )
  
  set.seed(1982)
  
  mod_rf_caret_leafnp_species <- train(
    pp,
    data            = df1h,
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafnp_species
  
  saveRDS(mod_rf_caret_leafnp_species, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "RF",
                 pred = "combined",
                 rsq = mod_rf_caret_leafnp_species$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafnp_species$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

##### Species-only

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafnp_species_phylo.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafnp_species_phylo <- readRDS(filn)
  
} else {
  
  ## one-hot encoding for Species identity (warning: 3700 species)
  df1h_phylo <- df %>% 
    recipe(LeafNP ~ ., data = dplyr::select(df, LeafNP, all_of(c("Species", "Family_New", "Genus")))) %>% 
    step_dummy(Species, one_hot = TRUE) %>% 
    step_dummy(Family_New, one_hot = TRUE) %>% 
    step_dummy(Genus, one_hot = TRUE) %>% 
    prep(training = df, retain = TRUE) %>% 
    juice()
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(LeafNP ~ ., data = df1h_phylo)

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = floor((ncol(df1h)-2) / 3), 
                            .min.node.size = 10,
                            .splitrule = "variance"
                            )
  # min.node.size  RMSE      Rsquared   MAE     
  #  5             6.306300  0.3440797  3.156061
  # 10             6.301759  0.3450017  3.155171
  # 30             6.303985  0.3443676  3.156464
    
  set.seed(1982)
  
  mod_rf_caret_leafnp_species_phylo <- train(
    pp,
    data            = df1h_phylo,
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafnp_species_phylo
  
  saveRDS(mod_rf_caret_leafnp_species_phylo, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "full",
                 model = "RF",
                 pred = "phylo",
                 rsq = mod_rf_caret_leafnp_species_phylo$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafnp_species_phylo$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

## Aggregated to sites

### Aggregate data

Load data aggregated to sites, done in `randomforest_leafnp.Rmd`.
```{r}
filn <- paste0(here::here(), "/data/dfs_leafnp_20210729.rds")

if (!file.exists(filn)){
  
  dfs <- df %>%

    ## remove unplausible leafNP data baed on Dis recommendation
    dplyr::filter(LeafNP < 70) %>%
  
    mutate(elv_grp = elv) %>%
    group_by(lon, lat, elv_grp, sitename) %>%
    summarise(across(c(preds, trgts), ~(mean(.x, na.rm = TRUE)))) %>%
    left_join(df %>%
                group_by(sitename) %>%
                summarise(nobs = n()),
              by = "sitename") %>%
    ungroup()
  
  saveRDS(dfs, file = filn)
  
} else {
  dfs <- readRDS(filn) 
}
```


### Leaf N

```{r}
target <- "leafN"
shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds"))
```

#### LM

```{r}
mod_lm_leafn <- lm(

  as.formula(paste("log(", target,") ~ ", paste(shortlist, collapse = " + "))),
  data = dplyr::select(dfs, leafN, all_of(shortlist)) %>% drop_na()

  )


# This is new released July 21 2021 with many indicators
tmp <- model_performance(mod_lm_leafn, metrics = "common", verbose = TRUE)
print(tmp)

# collect results into df
addrow <- tibble(target = target,
                 scale = "aggregated",
                 model = "LM",
                 pred = "combined",
                 rsq = tmp$R2,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

#### Random Forest

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafn.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafn <- readRDS(filn)
  
} else {
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafN ~ ., data = dplyr::select(dfs, leafN, all_of(shortlist))) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = 3,
                            .min.node.size = 5,
                            .splitrule = "variance"
                            )
  
#   mtry  min.node.size  RMSE      Rsquared   MAE     
#   2      5             4.884196  0.4479481  3.514411
#   2      8             4.896592  0.4455124  3.529126
#   2     10             4.919218  0.4405717  3.544740
#   2     30             5.007515  0.4218591  3.611234
#   3      5             4.870319  0.4503581  3.500935
#   3      8             4.881588  0.4481474  3.511033
#   3     10             4.890466  0.4463209  3.519473
#   3     30             4.988757  0.4248672  3.591211
#   4      5             4.871780  0.4497299  3.503742
#   4      8             4.882993  0.4472821  3.509671
#   4     10             4.893394  0.4451099  3.518867
#   4     30             4.968471  0.4289072  3.572633
#   5      5             4.878588  0.4480658  3.503187
#   5      8             4.879484  0.4479626  3.504207
#   5     10             4.894362  0.4447319  3.517260
#   5     30             4.968262  0.4284104  3.570183
# 
# Tuning parameter 'splitrule' was held constant at a value of variance
# RMSE was used to select the optimal model using the smallest value.
# The final values used for the model were mtry = 3, splitrule = variance and min.node.size = 5.
  
  set.seed(1982)
  
  mod_rf_caret_leafn <- train(
    pp,
    data            = dplyr::select(dfs, leafN, all_of(shortlist)),
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafn
  
  saveRDS(mod_rf_caret_leafn, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "aggregated",
                 model = "RF",
                 pred = "combined",
                 rsq = mod_rf_caret_leafn$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafn$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

### Leaf P

```{r}
target <- "leafP"
shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds"))
```

#### LM

```{r}
mod_lm_leafn <- lm(

  as.formula(paste("log(", target,") ~ ", paste(shortlist, collapse = " + "))),
  data = dplyr::select(dfs, leafP, all_of(shortlist)) %>% drop_na()

  )


# This is new released July 21 2021 with many indicators
tmp <- model_performance(mod_lm_leafn, metrics = "common", verbose = TRUE)
print(tmp)

# collect results into df
addrow <- tibble(target = target,
                 scale = "aggregated",
                 model = "LM",
                 pred = "combined",
                 rsq = tmp$R2,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

#### Random Forest

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafp.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafp <- readRDS(filn)
  
} else {
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafP ~ ., data = dplyr::select(dfs, leafP, all_of(shortlist))) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = 3, 
                            .min.node.size = 5,
                            .splitrule = "variance"
                            )

#   mtry  min.node.size  RMSE       Rsquared   MAE      
#   2      5             0.5042681  0.3357895  0.3243617
#   2      8             0.5051088  0.3342590  0.3247737
#   2     10             0.5056553  0.3332677  0.3251537
#   2     30             0.5128529  0.3166988  0.3304216
#   3      5             0.5034861  0.3373895  0.3240154
#   3      8             0.5042383  0.3358247  0.3246955
#   3     10             0.5046995  0.3346839  0.3249082
#   3     30             0.5116329  0.3181036  0.3294687
#   4      5             0.5044254  0.3346370  0.3250521
#   4      8             0.5042814  0.3352189  0.3245283
#   4     10             0.5051817  0.3329689  0.3252385
#   4     30             0.5107109  0.3195499  0.3286733
#   5      5             0.5043169  0.3348144  0.3243951
#   5      8             0.5052328  0.3324299  0.3257510
#   5     10             0.5055748  0.3317521  0.3256664
#   5     30             0.5106812  0.3189370  0.3288055
# 
# Tuning parameter 'splitrule' was held constant at a value of variance
# RMSE was used to select the optimal model using the smallest value.
# The final values used for the model were mtry = 3, splitrule = variance and min.node.size = 5.  
  
  set.seed(1982)
  
  mod_rf_caret_leafp <- train(
    pp,
    data            = dplyr::select(dfs, leafP, all_of(shortlist)),
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafp
  
  saveRDS(mod_rf_caret_leafp, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "aggregated",
                 model = "RF",
                 pred = "combined",
                 rsq = mod_rf_caret_leafp$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafp$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

### Leaf N:P

```{r}
target <- "LeafNP"
shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds"))
```

#### LM

```{r}
mod_lm_leafn <- lm(

  as.formula(paste("log(", target,") ~ ", paste(shortlist, collapse = " + "))),
  data = dplyr::select(dfs, LeafNP, all_of(shortlist)) %>% drop_na()

  )


# This is new released July 21 2021 with many indicators
tmp <- model_performance(mod_lm_leafn, metrics = "common", verbose = TRUE)
print(tmp)

# collect results into df
addrow <- tibble(target = target,
                 scale = "aggregated",
                 model = "LM",
                 pred = "combined",
                 rsq = tmp$R2,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

#### Random Forest

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafnp.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafnp <- readRDS(filn)
  
} else {
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(LeafNP ~ ., data = dplyr::select(dfs, LeafNP, all_of(shortlist))) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = 2, 
                            .min.node.size = 5,
                            .splitrule = "variance"
                            )
  
#   mtry  min.node.size  RMSE      Rsquared   MAE     
#   2      5             5.528117  0.2790776  3.442130
#   2      8             5.533388  0.2778906  3.442768
#   2     10             5.542267  0.2754941  3.451749
#   2     30             5.590654  0.2642769  3.489848
#   3      5             5.534519  0.2772633  3.445870
#   3      8             5.536107  0.2769081  3.450359
#   3     10             5.535879  0.2770679  3.451145
#   3     30             5.576416  0.2670492  3.474853
#   4      5             5.534420  0.2775807  3.449510
#   4      8             5.540016  0.2758562  3.451583
#   4     10             5.540370  0.2756434  3.450493
#   4     30             5.574360  0.2671489  3.473193
#   5      5             5.545552  0.2748855  3.458211
#   5      8             5.549643  0.2734559  3.459849
#   5     10             5.547744  0.2738793  3.460229
#   5     30             5.577375  0.2661357  3.476393
# 
# Tuning parameter 'splitrule' was held constant at a value of variance
# RMSE was used to select the optimal model using the smallest value.
# The final values used for the model were mtry = 2, splitrule = variance and min.node.size = 5.
    
  set.seed(1982)
  
  mod_rf_caret_leafnp <- train(
    pp,
    data            = dplyr::select(dfs, LeafNP, all_of(shortlist)),
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafnp
  
  saveRDS(mod_rf_caret_leafnp, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "aggregated",
                 model = "RF",
                 pred = "combined",
                 rsq = mod_rf_caret_leafnp$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafnp$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

## Modelled vs observed

### Leaf N

Visualise cross-validation results using results from the best tuned model.
```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafn.rds")
mod_rf_caret_leafn <- readRDS(filn)

## get predicted values from cross-validation resamples, take mean across repetitions
df_cv <- mod_rf_caret_leafn$pred %>% 
  as_tibble() %>% 
  dplyr::filter(mtry == mod_rf_caret_leafn$bestTune$mtry, 
                splitrule == mod_rf_caret_leafn$bestTune$splitrule, 
                min.node.size == mod_rf_caret_leafn$bestTune$min.node.size) %>%
  separate(Resample, into = c(NA, "Fold"), sep = "old") %>% 
  dplyr::rename(idx = rowIndex)

# contains the same values as dplyr::select(dfs, leafN, all_of(shortlist)) ?
target <- "leafN"
shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds"))
tmp <- dplyr::select(dfs, leafN, all_of(shortlist)) |> 
  mutate(idx = 1:nrow(dfs)) |> 
  right_join(
    df_cv,
    by = "idx"
  )

# correct
out <- tmp |> 
  rbeni::analyse_modobs2("leafN", "obs")
out$gg

out <- df_cv %>% 
  rbeni::analyse_modobs2("pred", "obs", type = "heat", shortsubtitle = TRUE)
gg1 <- out$gg +
  ylim(5,40) + xlim(5,40) +
  labs(x = "Predicted leaf N (mg/g)", y = "Observed leaf N (mg/g)")
gg1
saveRDS(gg1, file = paste0(here::here(), "/data/gg1.rds"))
write_csv(df_cv, file = paste0(here::here(), "/data/df_cv_leafn.csv"))
```

### Leaf P

Visualise cross-validation results using results from the best tuned model.
```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafp.rds")
mod_rf_caret_leafp <- readRDS(filn)

## get predicted values from cross-validation resamples, take mean across repetitions
df_cv <- mod_rf_caret_leafp$pred %>% 
  as_tibble() %>% 
  dplyr::filter(mtry == mod_rf_caret_leafp$bestTune$mtry, 
                splitrule == mod_rf_caret_leafp$bestTune$splitrule, 
                min.node.size == mod_rf_caret_leafp$bestTune$min.node.size) %>%
  separate(Resample, into = c(NA, "Fold"), sep = "old") %>% 
  dplyr::rename(idx = rowIndex)

out <- df_cv %>% 
  rbeni::analyse_modobs2("pred", "obs", type = "heat", shortsubtitle = TRUE)
gg2 <- out$gg +
  ylim(0, 4) + xlim(0, 4) +
  labs(x = "Predicted leaf N (mg/g)", y = "Observed leaf N (mg/g)")
gg2
saveRDS(gg2, file = paste0(here::here(), "/data/gg2.rds"))
write_csv(df_cv, file = paste0(here::here(), "/data/df_cv_leafp.csv"))
```

### Leaf N:P

Visualise cross-validation results using results from the best tuned model.
```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafnp.rds")
mod_rf_caret_leafnp <- readRDS(filn)

## get predicted values from cross-validation resamples, take mean across repetitions
df_cv <- mod_rf_caret_leafnp$pred %>% 
  as_tibble() %>% 
  dplyr::filter(mtry == mod_rf_caret_leafnp$bestTune$mtry, 
                splitrule == mod_rf_caret_leafnp$bestTune$splitrule, 
                min.node.size == mod_rf_caret_leafnp$bestTune$min.node.size) %>%
  separate(Resample, into = c(NA, "Fold"), sep = "old") %>% 
  dplyr::rename(idx = rowIndex)

out <- df_cv %>% 
  rbeni::analyse_modobs2("pred", "obs", type = "heat", shortsubtitle = TRUE)
gg3 <- out$gg +
  ylim(0, 40) + xlim(0, 40) +
  labs(x = "Predicted leaf N (mg/g)", y = "Observed leaf N (mg/g)")
gg3
saveRDS(gg3, file = paste0(here::here(), "/data/gg3.rds"))
write_csv(df_cv, file = paste0(here::here(), "/data/df_cv_leafnp.csv"))
```

## Aggregated to ecoregions

Add ecoregion name to site-level data
```{r}
filn <- paste0(here::here(), "/data/df_ecoregion.rds")

if (!file.exists(filn)){

  ## add WWF biome info
  dfs_wwf <- ingestr::ingest(
    dfs|> 
      dplyr::select(sitename, lon, lat),
    source = "wwf",
    dir = "~/data/biomes/wwf_ecoregions/official/",
    settings = list(layer = "wwf_terr_ecos")
    )|> 
    mutate(data = purrr::map(data, ~slice(., 1)))|> 
    unnest(data) |> 
    rename(biome_wwf = BIOME_NAME) |> 
    # left_join(df_biome_id, by = "biome_wwf") |> 
    right_join(dfs, by = "sitename")
  
} else {
  dfs_wwf <- readRDS(filn)
}

df_wwf <- dfs_wwf |> 
  select(sitename, ECO_NAME) |> 
  distinct() |> 
  right_join(
    df, 
    by = "sitename"
  )


use_ecoregions <- df_wwf |> 
  group_by(ECO_NAME) |> 
  summarise(n = n()) |> 
  arrange(desc(n)) |> 
  filter(n > 30) |> 
  drop_na() |> 
  pull(ECO_NAME)

df_wwf <- df_wwf |> 
  filter(ECO_NAME %in% use_ecoregions) |> 
  group_by(ECO_NAME) %>%
  summarise(across(c(preds, trgts), ~(mean(.x, na.rm = TRUE))))
```

### Leaf N

```{r}
target <- "leafN"
shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds"))
```

#### LM

```{r}
mod_lm_leafn <- lm(

  as.formula(paste("log(", target,") ~ ", paste(shortlist, collapse = " + "))),
  data = dplyr::select(df_wwf, leafN, all_of(shortlist)) %>% drop_na()

  )


# This is new released July 21 2021 with many indicators
tmp <- model_performance(mod_lm_leafn, metrics = "common", verbose = TRUE)
print(tmp)

# collect results into df
addrow <- tibble(target = target,
                 scale = "ecoregions",
                 model = "LM",
                 pred = "combined",
                 rsq = tmp$R2,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

#### Random Forest

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafn_ecoregions.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafn <- readRDS(filn)
  
} else {
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafN ~ ., data = dplyr::select(df_wwf, leafN, all_of(shortlist))) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = 3,
                            .min.node.size = 5,
                            .splitrule = "variance"
                            )

# Tuning parameter 'splitrule' was held constant at a value of variance
# RMSE was used to select the optimal model using the smallest value.
# The final values used for the model were mtry = 3, splitrule = variance and min.node.size = 5.
  
  set.seed(1982)
  
  mod_rf_caret_leafn <- train(
    pp,
    data            = dplyr::select(df_wwf, leafN, all_of(shortlist)),
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafn
  
  saveRDS(mod_rf_caret_leafn, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "ecoregions",
                 model = "RF",
                 pred = "combined",
                 rsq = mod_rf_caret_leafn$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafn$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```



<!-- ## Morphoclimatic groups -->

<!-- ### Get data -->

<!-- Extract biome information from WWF and NCRS biome map. -->
<!-- ```{r eval=FALSE} -->
<!-- filn <- paste0(here::here(), "/data/dfs_morphoclim.rds") -->

<!-- if (!file.exists(filn)){ -->

<!--   df_biome_id <- read_csv(paste0(here::here(), "/data/biome_id.csv")) -->

<!--   ## add WWF biome info -->
<!--   dfs_wwf <- ingestr::ingest( -->
<!--     dfs|>  -->
<!--       dplyr::select(sitename, lon, lat), -->
<!--     source = "wwf", -->
<!--     dir = "~/data/biomes/wwf_ecoregions/official/", -->
<!--     settings = list(layer = "wwf_terr_ecos") -->
<!--     )|>  -->
<!--     mutate(data = purrr::map(data, ~slice(., 1)))|>  -->
<!--     unnest(data) |>  -->
<!--     rename(biome_wwf = BIOME_NAME) |>  -->
<!--     left_join(df_biome_id, by = "biome_wwf") |>  -->
<!--     right_join(dfs, by = "sitename") -->

<!--   saveRDS(dfs_wwf, file = filn) -->

<!-- } else { -->
<!--   dfs_wwf <- readRDS(filn) -->
<!-- } -->
<!-- ``` -->

<!-- Consider only biomes with enough data (N > 100). -->
<!-- ```{r} -->
<!-- use_biomes <- dfs_wwf |>  -->
<!--   group_by(mybiome) |>  -->
<!--   summarise(n = n()) |>  -->
<!--   filter(n > 100) |>  -->
<!--   pull(mybiome) -->
<!-- ``` -->

<!-- Subset. -->
<!-- ```{r} -->
<!-- df_wwf_sub <- dfs_wwf |>  -->
<!--   filter(mybiome %in% use_biomes) |>  -->
<!--   ungroup() -->
<!-- ``` -->

<!-- ### Number of species per biome -->

<!-- Analyse number of distinct species by biome. -->
<!-- ```{r} -->
<!-- df |>  -->
<!--   left_join( -->
<!--     select(df_wwf_sub, sitename, mybiome), -->
<!--     by = "sitename" -->
<!--   ) |>  -->
<!--   select(mybiome, Species) |>  -->
<!--   distinct() |>  -->
<!--   group_by(mybiome) |>  -->
<!--   summarise(n = n()) |>  -->
<!--   arrange(desc(n)) |>  -->
<!--   drop_na() -->

<!-- # just boreal -->
<!-- df |>  -->
<!--   left_join( -->
<!--     select(df_wwf_sub, sitename, mybiome), -->
<!--     by = "sitename" -->
<!--   ) |>  -->
<!--   filter(mybiome == "Boreal") |>  -->
<!--   group_by(Species) |>  -->
<!--   summarise(n = n()) |>  -->
<!--   arrange(desc(n)) -->
<!-- ``` -->


<!-- ### Fit model -->

<!-- ```{r} -->
<!-- target <- "leafN" -->
<!-- shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds")) -->

<!-- filn <- paste0(here::here(), "/data/mod_rf_caret_leafn_morphoclim.rds") -->

<!-- if (file.exists(filn) && !overwrite){ -->

<!--   mod_rf_caret_leafn_morphoclim <- readRDS(filn) -->

<!-- } else { -->

<!--   ## one-hot encoding for Species identity (warning: 3700 species) -->
<!--   df1h <- df_wwf_sub %>%  -->
<!--     recipe(leafN ~ .,  -->
<!--            data = dplyr::select(df_wwf_sub,  -->
<!--                                 leafN,  -->
<!--                                 all_of(c(shortlist, "mybiome")))) %>%  -->
<!--     step_dummy(mybiome, one_hot = TRUE) %>%  -->
<!--     prep(training = df_wwf_sub, retain = TRUE) %>%  -->
<!--     juice() -->

<!--   ## create generic formula for the model and define preprocessing steps -->
<!--   pp <- recipe(leafN ~ ., data = df1h) %>% -->

<!--     ## impute by median as part of the recipe -->
<!--     step_impute_median(all_predictors()) -->


<!--   traincotrlParams <- trainControl(  -->
<!--     method = "cv",  -->
<!--     number = 5,  -->
<!--     verboseIter = FALSE, -->
<!--     savePredictions = "final" -->
<!--     ) -->

<!--   ## best choice -->
<!--   tune_grid <- expand.grid( .mtry = floor((ncol(df1h)-1) / 3), -->
<!--                             .min.node.size = 5, -->
<!--                             .splitrule = "variance" -->
<!--                             ) -->

<!-- #   RMSE      Rsquared   MAE      -->
<!-- #   4.864925  0.4511468  3.486146 -->
<!-- #  -->
<!-- # Tuning parameter 'mtry' was held constant at a value of 5 -->
<!-- # Tuning parameter 'splitrule' was held constant at -->
<!-- #  a value of variance -->
<!-- # Tuning parameter 'min.node.size' was held constant at a value of 5 -->
<!-- #   set.seed(1982) -->

<!--   mod_rf_caret_leafn_morphoclim <- train( -->
<!--     pp, -->
<!--     data            = df1h, -->
<!--     metric          = "RMSE", -->
<!--     method          = "ranger", -->
<!--     tuneGrid        = tune_grid, -->
<!--     trControl       = traincotrlParams, -->
<!--     replace         = TRUE, -->
<!--     sample.fraction = 0.5, -->
<!--     num.trees       = 2000,        # boosted for the final model -->
<!--     importance      = "impurity"   # for variable importance analysis, alternative: "permutation" -->
<!--     ) -->

<!--   saveRDS(mod_rf_caret_leafn_morphoclim, file = filn) -->

<!-- } -->

<!-- # model results -->
<!-- mod_rf_caret_leafn_morphoclim -->
<!-- ``` -->

## Within-species variations only

```{r}
use_species <- df |>
  group_by(Species) |> 
  summarise(n = n()) |>
  arrange(desc(n)) |> 
  filter(n > 5) |> 
  pull(Species)

df_species <- df |>
  
  # subset
  filter(Species %in% use_species) |> 
  dplyr::select(all_of(c("sitename", "Species", "leafN", "leafP", "LeafNP"))) |> 

  # fit linear regressions by species
  group_by(Species) |> 
  summarise(across(all_of(c("leafN", "leafP", "LeafNP")), mean)) |> 
  rename(leafN_mean = leafN, leafP_mean = leafP, leafNP_mean = LeafNP)

df_species_mean <- df_species |> 
  ungroup() |> 
  summarise(across(all_of(c("leafN_mean", "leafP_mean", "leafNP_mean")), mean))
  
df2 <- df |> 

  # subset
  filter(Species %in% use_species) |> 
  
  left_join(df_species, by = "Species") |> 
  ungroup() |> 
  mutate(leafN_diff = leafN - leafN_mean, 
         leafP_diff = leafP - leafP_mean, 
         leafNP_diff = LeafNP - leafNP_mean)
```

### Leaf N

```{r}
target <- "leafN"
shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds"))
```

#### LM

```{r}
mod_lm_leafn_within <- lm(

  as.formula(paste("log( leafN_diff ) ~ ", paste(shortlist, collapse = " + "))),
  data = dplyr::select(df2, leafN_diff, all_of(shortlist)) %>% drop_na()

  )


# This is new released July 21 2021 with many indicators
tmp <- model_performance(mod_lm_leafn_within, metrics = "common", verbose = TRUE)
print(tmp)

# collect results into df
addrow <- tibble(target = target,
                 scale = "within",
                 model = "LM",
                 pred = "environment",
                 rsq = tmp$R2,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

#### Random Forest

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafn_within.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafn_within <- readRDS(filn)
  
} else {
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafN_diff ~ ., data = dplyr::select(df2, leafN_diff, all_of(shortlist))) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = 3,
                            .min.node.size = 8,
                            .splitrule = "variance"
                            )
  
#   mtry  min.node.size  RMSE      Rsquared   MAE     
#   2      3             3.034099  0.2908401  1.954698
#   2      5             3.030944  0.2915255  1.954106
#   2      8             3.031705  0.2904935  1.955953
#   2     10             3.032102  0.2899871  1.957563
#   2     30             3.053492  0.2807764  1.975929
#   3      3             3.032462  0.2918925  1.955163
#   3      5             3.031750  0.2916066  1.955047
#   3      8             3.030372  0.2914033  1.954893
#   3     10             3.031725  0.2904056  1.956186
#   3     30             3.048420  0.2826194  1.971113
#   4      3             3.035259  0.2909028  1.956637
#   4      5             3.033229  0.2911054  1.955840
#   4      8             3.031759  0.2910232  1.955106
#   4     10             3.031150  0.2908795  1.956275
#   4     30             3.045470  0.2837563  1.969138
#   5      3             3.035900  0.2907365  1.956152
#   5      5             3.033462  0.2912024  1.955757
#   5      8             3.032381  0.2909391  1.956735
#   5     10             3.031754  0.2907095  1.956024
#   5     30             3.044440  0.2841162  1.968234
# 
# Tuning parameter 'splitrule' was held constant at a value of variance
# RMSE was used to select the optimal model using the smallest value.
# The final values used for the model were mtry = 3, splitrule = variance and min.node.size = 8.
  
  set.seed(1982)
  
  mod_rf_caret_leafn_within <- train(
    pp,
    data            = dplyr::select(df2, leafN_diff, all_of(shortlist)),
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafn_within
  
  saveRDS(mod_rf_caret_leafn_within, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "within",
                 model = "RF",
                 pred = "environment",
                 rsq = mod_rf_caret_leafn_within$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafn_within$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

### Leaf P

```{r}
target <- "leafP"
shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds"))
```

#### LM

```{r}
mod_lm_leafp_within <- lm(

  as.formula(paste("log( leafP_diff ) ~ ", paste(shortlist, collapse = " + "))),
  data = dplyr::select(df2, leafP_diff, all_of(shortlist)) %>% drop_na()

  )


# This is new released July 21 2021 with many indicators
tmp <- model_performance(mod_lm_leafp_within, metrics = "common", verbose = TRUE)
print(tmp)

# collect results into df
addrow <- tibble(target = target,
                 scale = "within",
                 model = "LM",
                 pred = "environment",
                 rsq = tmp$R2,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

#### Random Forest

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafp_within.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafp_within <- readRDS(filn)
  
} else {
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafP_diff ~ ., data = dplyr::select(df2, leafP_diff, all_of(shortlist))) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = 3,
                            .min.node.size = 3,
                            .splitrule = "variance"
                            )
  
#   mtry  min.node.size  RMSE       Rsquared   MAE      
#   2      3             0.3810881  0.3084349  0.2415511
#   2      5             0.3811464  0.3078083  0.2416437
#   2      8             0.3817988  0.3051723  0.2419735
#   2     10             0.3825964  0.3022465  0.2424457
#   2     30             0.3879025  0.2861176  0.2465989
#   3      3             0.3805937  0.3107140  0.2415331
#   3      5             0.3807597  0.3095421  0.2415571
#   3      8             0.3816104  0.3059920  0.2419929
#   3     10             0.3817637  0.3052921  0.2421188
#   3     30             0.3864996  0.2900589  0.2455509
#   4      3             0.3809152  0.3098003  0.2418271
#   4      5             0.3807967  0.3096323  0.2416590
#   4      8             0.3812495  0.3074272  0.2419214
#   4     10             0.3817831  0.3053258  0.2421721
#   4     30             0.3860038  0.2912901  0.2451559
#   5      3             0.3810601  0.3094426  0.2418946
#   5      5             0.3810532  0.3089526  0.2418074
#   5      8             0.3815404  0.3065569  0.2422402
#   5     10             0.3818055  0.3053505  0.2422428
#   5     30             0.3857816  0.2916508  0.2449964
# 
# Tuning parameter 'splitrule' was held constant at a value of variance
# RMSE was used to select the optimal model using the smallest value.
# The final values used for the model were mtry = 3, splitrule = variance and min.node.size = 3.  
  
  set.seed(1982)
  
  mod_rf_caret_leafp_within <- train(
    pp,
    data            = dplyr::select(df2, leafP_diff, all_of(shortlist)),
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafp_within
  
  saveRDS(mod_rf_caret_leafp_within, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "within",
                 model = "RF",
                 pred = "environment",
                 rsq = mod_rf_caret_leafp_within$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafp_within$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```


### Leaf N:P

```{r}
target <- "leafNP"
shortlist <- readRDS(paste0(here::here(), "/data/shortlist_", target, ".rds"))
```

#### LM

```{r}
mod_lm_leafnp_within <- lm(

  as.formula(paste("log( leafNP_diff ) ~ ", paste(shortlist, collapse = " + "))),
  data = dplyr::select(df2, leafNP_diff, all_of(shortlist)) %>% drop_na()

  )


# This is new released July 21 2021 with many indicators
tmp <- model_performance(mod_lm_leafnp_within, metrics = "common", verbose = TRUE)
print(tmp)

# collect results into df
addrow <- tibble(target = target,
                 scale = "within",
                 model = "LM",
                 pred = "environment",
                 rsq = tmp$R2,
                 rmse = tmp$RMSE)
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```

#### Random Forest

```{r}
filn <- paste0(here::here(), "/data/mod_rf_caret_leafnp_within.rds")

if (file.exists(filn) && !overwrite){
  
  mod_rf_caret_leafnp_within <- readRDS(filn)
  
} else {
 
  ## create generic formula for the model and define preprocessing steps
  pp <- recipe(leafNP_diff ~ ., data = dplyr::select(df2, leafNP_diff, all_of(shortlist))) %>%
  
    ## impute by median as part of the recipe
    # step_medianimpute(all_predictors())
    step_impute_median(all_predictors())

  traincotrlParams <- trainControl( 
    method = "cv", 
    number = 5, 
    verboseIter = FALSE,
    savePredictions = "final"
    )
  
  ## best choice
  tune_grid <- expand.grid( .mtry = 2,
                            .min.node.size = 10,
                            .splitrule = "variance"
                            )
  
#   mtry  min.node.size  RMSE      Rsquared   MAE     
#   2      3             5.548467  0.2150808  2.452213
#   2      5             5.551590  0.2135746  2.455358
#   2      8             5.547519  0.2136578  2.455343
#   2     10             5.538746  0.2154857  2.452458
#   2     30             5.567315  0.2058594  2.486075
#   3      3             5.567058  0.2109273  2.456633
#   3      5             5.556843  0.2127068  2.454572
#   3      8             5.550123  0.2134689  2.455857
#   3     10             5.558640  0.2106617  2.457978
#   3     30             5.578583  0.2015879  2.485177
#   4      3             5.567234  0.2110802  2.458628
#   4      5             5.586666  0.2056069  2.461756
#   4      8             5.565188  0.2093511  2.457767
#   4     10             5.577756  0.2058568  2.459492
#   4     30             5.567782  0.2048737  2.479242
#   5      3             5.566441  0.2113829  2.454717
#   5      5             5.585496  0.2058519  2.459852
#   5      8             5.567774  0.2093039  2.458472
#   5     10             5.563416  0.2095856  2.459791
#   5     30             5.582647  0.2004493  2.481802
# 
# Tuning parameter 'splitrule' was held constant at a value of variance
# RMSE was used to select the optimal model using the smallest value.
# The final values used for the model were mtry = 2, splitrule = variance and min.node.size = 10.  

  set.seed(1982)
  
  mod_rf_caret_leafnp_within <- train(
    pp,
    data            = dplyr::select(df2, leafNP_diff, all_of(shortlist)),
    metric          = "RMSE",
    method          = "ranger",
    tuneGrid        = tune_grid,
    trControl       = traincotrlParams,
    replace         = TRUE,
    sample.fraction = 0.5,
    num.trees       = 2000,        # boosted for the final model
    importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
  
  mod_rf_caret_leafnp_within
  
  saveRDS(mod_rf_caret_leafnp_within, file = filn)
}

# collect results into df
addrow <- tibble(target = target,
                 scale = "within",
                 model = "RF",
                 pred = "environment",
                 rsq = mod_rf_caret_leafnp_within$results[["Rsquared"]],
                 rmse = mod_rf_caret_leafnp_within$results[["RMSE"]])
df_model_fitting <- df_model_fitting %>% 
  bind_rows(addrow)
```


## Visualisation

### Overview

Explanation:

For the data aggregated to site-level (a, c, d), the R2 represents the coefficient of determination on a multivariate linear regression model (LM), and the mean across a 5-fold cross validation for the RF. Only environmental factors are considered as model predictors. 

For the non-aggregated, species-level data, The top edge of the bar represents the conditional R-squared for the linear mixed effects models (LMM) and the mean across a 5-fold cross validation for the RF, using environmental and phylogenetic variables as predictors. Species identity is considered as the grouping variable for random intercepts in the LMM and is considered as a one-hot encoded predictor in the RF. The "phylogenetic" portion (brown bar) visualises the the intraclass correlation coefficient (ICC) for the LMMs, determined on the same model and representing the proportion of the variance explained by species identity. For the RF models, the "phylogenetic" pportion of the bar represents the R-quared (mean across 5-fold cross validation) of a model that contains only species, genus, and family identity (one-hot encoded) as predictors.

```{r}
# N
gg1 <- df_model_fitting %>% 
  filter(target == "leafN" & scale == "aggregated") %>% 
  ggplot(aes(x = model, y = rsq)) +
  geom_bar(stat="identity", position = "identity", fill = "#29a274ff") +
  theme_classic() +
  labs(title ="Leaf N", subtitle = "Site-level data", y = expression(italic(R)^2), x = "") +
  scale_y_continuous(limits = c(0, 0.8), expand = c(0, 0))

gg2 <- df_model_fitting %>% 
  filter(target == "leafN" & scale == "full") %>% 
  ggplot(aes(x = model, y = rsq, fill = pred)) +
  geom_bar(stat="identity", position = "identity") +
  theme_classic() +
  labs(title ="Leaf N", subtitle = "Full data", y = expression(italic(R)^2), x = "") +
  scale_fill_manual(values = c("#29a274ff", "#777055ff"), labels = c("Environment", "Species identity"), name = NULL) +
  scale_y_continuous(limits = c(0, 0.8), expand = c(0, 0))

gg2_5 <- df_model_fitting %>% 
  filter(target == "leafN" & scale == "within") %>% 
  ggplot(aes(x = model, y = rsq)) +
  geom_bar(stat="identity", position = "identity", fill = "#29a274ff") +
  theme_classic() +
  labs(title ="Leaf N", subtitle = "Within-species only", y = expression(italic(R)^2), x = "") +
  scale_y_continuous(limits = c(0, 0.8), expand = c(0, 0))

# P
gg3 <- df_model_fitting %>% 
  filter(target == "leafP" & scale == "aggregated") %>% 
  ggplot(aes(x = model, y = rsq)) +
  geom_bar(stat="identity", position = "identity", fill = "#29a274ff") +
  theme_classic() +
  labs(title ="Leaf P", subtitle = "Site-level data", y = expression(italic(R)^2), x = "") +
  scale_y_continuous(limits = c(0, 0.8), expand = c(0, 0))

gg4 <- df_model_fitting %>% 
  filter(target == "leafP" & scale == "full") %>% 
  ggplot(aes(x = model, y = rsq, fill = pred)) +
  geom_bar(stat="identity", position = "identity") +
  theme_classic() +
  labs(title ="Leaf P", subtitle = "Full data", y = expression(italic(R)^2), x = "") +
  scale_fill_manual(values = c("#29a274ff", "#777055ff"), labels = c("Environment", "Species identity"), name = NULL) +
  scale_y_continuous(limits = c(0, 0.8), expand = c(0, 0))

gg4_5 <- df_model_fitting %>% 
  filter(target == "leafP" & scale == "within") %>% 
  ggplot(aes(x = model, y = rsq)) +
  geom_bar(stat="identity", position = "identity", fill = "#29a274ff") +
  theme_classic() +
  labs(title ="Leaf P", subtitle = "Within-species only", y = expression(italic(R)^2), x = "") +
  scale_y_continuous(limits = c(0, 0.8), expand = c(0, 0))

# N:P
gg5 <- df_model_fitting %>% 
  filter(target == "LeafNP" & scale == "aggregated") %>% 
  ggplot(aes(x = model, y = rsq)) +
  geom_bar(stat="identity", position = "identity", fill = "#29a274ff") +
  theme_classic() +
  labs(title ="Leaf N:P", subtitle = "Site-level data", y = expression(italic(R)^2), x = "") +
  scale_y_continuous(limits = c(0, 0.8), expand = c(0, 0))

gg6 <- df_model_fitting %>% 
  filter(target == "LeafNP" & scale == "full") %>% 
  ggplot(aes(x = model, y = rsq, fill = pred)) +
  geom_bar(stat="identity", position = "identity") +
  theme_classic() +
  labs(title ="Leaf N:P", subtitle = "Full data", y = expression(italic(R)^2), x = "") +
  scale_fill_manual(values = c("#29a274ff", "#777055ff"), labels = c("Environment", "Species identity"), name = NULL) +
  scale_y_continuous(limits = c(0, 0.8), expand = c(0, 0))

gg6_5 <- df_model_fitting %>% 
  filter(target == "leafNP" & scale == "within") %>% 
  ggplot(aes(x = model, y = rsq)) +
  geom_bar(stat="identity", position = "identity", fill = "#29a274ff") +
  theme_classic() +
  labs(title ="Leaf N:P", subtitle = "Within-species only", y = expression(italic(R)^2), x = "") +
  scale_y_continuous(limits = c(0, 0.8), expand = c(0, 0))

plot_grid(gg1, gg2, rel_widths = c(0.5, 0.8), ncol = 2)
plot_grid(gg3, gg4, rel_widths = c(0.5, 0.8), ncol = 2)
plot_grid(gg5, gg6, rel_widths = c(0.5, 0.8), ncol = 2)

plot_grid(gg1, gg2_5, gg2, 
          gg3, gg4_5, gg4,
          gg5, gg6_5, gg6,
          rel_widths = c(0.5, 0.5, 0.8), ncol = 3, labels = c("a", "b", "c", "d", "e", "f", "g", "h", "i"))
ggsave(paste0(here::here(), "/fig/bars_model_fitting.pdf"), width = 9, height = 8)
```

### Effects

```{r}
df_t_n <- tibble(
  pred = names(summary(mod_lmm_leafn)$tTable[,"t-value"]),
  tval = summary(mod_lmm_leafn)$tTable[,"t-value"],
  pval = summary(mod_lmm_leafn)$tTable[,"p-value"]
  ) |> 
  filter(pred != "(Intercept)") |> 
  filter(pval < 0.01)

df_t_p <- tibble(
  pred = names(summary(mod_lmm_leafp)$tTable[,"t-value"]),
  tval = summary(mod_lmm_leafp)$tTable[,"t-value"],
  pval = summary(mod_lmm_leafp)$tTable[,"p-value"]
  ) |> 
  filter(pred != "(Intercept)") |> 
  filter(pval < 0.01)

df_t_np <- tibble(
  pred = names(summary(mod_lmm_leafnp)$tTable[,"t-value"]),
  tval = summary(mod_lmm_leafnp)$tTable[,"t-value"],
  pval = summary(mod_lmm_leafnp)$tTable[,"p-value"]
  ) |> 
  filter(pred != "(Intercept)") |> 
  filter(pval < 0.01)
```

```{r}
ggt1 <- df_t_n %>% 
  mutate(pred = fct_reorder(pred, tval)) %>%
  ggplot(aes(pred, tval)) +
  geom_bar(stat = "identity", fill = "#777055ff") +
  labs(title = "Leaf N", x = "", y = expression(italic(t)-value)) +
  coord_flip() +
  theme_classic()

ggt2 <- df_t_p %>% 
  mutate(pred = fct_reorder(pred, tval)) %>%
  ggplot(aes(pred, tval)) +
  geom_bar(stat = "identity", fill = "#777055ff") +
  labs(title = "Leaf P", x = "", y = expression(italic(t)-value)) +
  coord_flip() +
  theme_classic()

ggt3 <- df_t_np %>% 
  mutate(pred = fct_reorder(pred, tval)) %>%
  ggplot(aes(pred, tval)) +
  geom_bar(stat = "identity", fill = "#777055ff") +
  labs(title = "Leaf N:P", x = "", y = expression(italic(t)-value)) +
  coord_flip() +
  theme_classic()

# files created in feature_elimination_leafnp.Rmd
gga1 <- readRDS(paste0(here::here(), "/data/gga1.rds"))
gga2 <- readRDS(paste0(here::here(), "/data/gga2.rds"))
gga3 <- readRDS(paste0(here::here(), "/data/gga3.rds"))

plot_grid(gga1, gga2, gga3,
          ggt1, ggt2, ggt3, 
          labels =  c("a", "b", "c", "d", "e", "f"), 
          ncol = 3,
          rel_heights = c(1, 0.65))

ggsave(paste0(here::here(), "/fig/bars_fe_tvals.pdf"), width = 12, height = 6)
```
