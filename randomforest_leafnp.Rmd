---
title: "Random Forest for leafnp"
author: "Beni Stocker"
date: "5/6/2021"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ranger)
library(caret)
library(visdat)
library(vip)
library(pdp)
library(nnet)
library(recipes)
```

## Read data

```{r}
# df <- read_csv("~/data/LeafNP_tiandi/Global_total_leaf_N_P_Di/soil_property_extraction_20210323/global_leaf_NP_with_soil_property_from_HWSD_WISE_GSDE_Pmodel_Ndep_GTI_CO2_25032021.csv") %>% 
#   mutate(grass = tree_shrub_Herb == "H") 

df <- read_csv("~/data/LeafNP_tiandi/Global_total_leaf_N_P_Di/leafnp_data_covariates_20210702.csv") %>% 
  mutate(grass = tree_shrub_Herb == "H")

trgts <- c("leafN", "leafP", "LeafNP")

## predictors excluding PHO, and TS (too many missing)
preds <- c("elv", "mat", "matgs", "tmonthmin", "tmonthmax", "ndaysgs", "mai", "maigs", "map", "pmonthmin", "mapgs", "mavgs", "mav", "alpha", "vcmax25", "jmax25", "gs_accl", "aet", "ai", "cwdx80", "gti", "ndep", "co2", "T_BULK_DENSITY", "AWC_CLASS", "T_CLAY", "T_SILT", "T_SAND", "T_GRAVEL", "T_PH_H2O", "T_TEB", "T_BS", "T_CEC_SOIL", "T_CEC_CLAY", "T_ECE", "T_ESP", "T_CACO3", "T_OC", "ORGC", "TOTN", "CNrt", "ALSA", "PBR", "TP", "TK")

preds_soil <- c("T_BULK_DENSITY", "AWC_CLASS", "T_CLAY", "T_SILT", "T_SAND", "T_GRAVEL", "T_PH_H2O", "T_TEB", "T_BS", "T_CEC_SOIL", "T_CEC_CLAY", "T_ECE", "T_ESP", "T_CACO3", "T_OC", "ORGC", "TOTN", "CNrt", "ALSA", "PBR", "TP", "TK")
  
preds_climate <- c("mat", "matgs", "tmonthmin", "tmonthmax", "ndaysgs", "mai", "maigs", "map", "pmonthmin", "mapgs", "mavgs", "mav", "alpha", "vcmax25", "jmax25", "gs_accl", "aet", "ai", "cwdx80")

preds_other <- c("elv", "gti", "ndep", "co2")

preds_pmodeloutputs <- c("vcmax25", "jmax25", "gs_accl")

preds_pmodelinputs <- c("mat", "matgs", "mai", "maigs", "mav", "mavgs", "elv", "co2")

vis_miss(df %>% dplyr::select(all_of(trgts), all_of(preds)), warn_large_data = F)
```

Look at some values
```{r}
df %>% 
  mutate(grass = tree_shrub_Herb == "H") %>% 
  ggplot(aes(x = leafN, y = ..density.., fill = grass)) + 
  geom_histogram(position="identity", alpha = 0.5)
```

```{r}
df %>% 
  ggplot(aes(vcmax25, leafN)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "lm")
```

Take site means.
```{r}
dfs <- df %>% 
  # # xxx test: only grasslands
  # dplyr::filter(grass) %>% 
  
  mutate(elv_grp = elv) %>% 
  group_by(lon, lat, elv_grp, sitename) %>% 
  summarise(across(c(preds, trgts), ~(mean(.x, na.rm = TRUE)))) %>% 
  left_join(df %>% 
              group_by(sitename) %>% 
              summarise(nobs = n()),
            by = "sitename") %>% 
  ungroup()

saveRDS(dfs, file = "data/dfs_leafnp.rds")
```

Visualise missing data.
```{r}
vis_miss(dfs %>% dplyr::select(trgts, preds), warn_large_data = F)
```
There are a lot of data points still missing, especially for HWSD data and PFB, and also alpha. Am I using the latest updated dataset? 

Use only data from sites with at least three observations. Reduces it from 7545 to 2200 points.
```{r}
# dfs <- dfs %>% 
#   dplyr::filter(nobs >= 3)
```


## Train a model

### Ranger

A random forest model using the ranger library.

Out of the box, it gets R2 = 0.46.
```{r}
mod_rf_leafn <- ranger(
  leafN ~ ., 
  data = dfs %>% 
    dplyr::select(leafN, all_of(preds)) %>% 
    drop_na(),
  mtry = floor(length(preds) / 3),
  respect.unordered.factors = "order",
  seed = 123
)

## RMSE and R2
sqrt(mod_rf_leafn$prediction.error)
mod_rf_leafn$r.squared
```

Does it perform better without `PBR` as predictor (too limiting because of too many missing data points)? This leaves 6859 points in the dataset (as opposed to 5922 points when PBR is included).
Yes! It works better, indeed: R2 = 0.48.
```{r}
mod_rf_leafn_noPBR <- ranger(
  leafN ~ ., 
  data = dfs %>% 
    dplyr::select(leafN, all_of(preds)) %>% 
    dplyr::select(-PBR) %>% 
    drop_na(),
  mtry = floor((length(preds)-1) / 3),
  respect.unordered.factors = "order",
  seed = 123
)

## RMSE and R2
sqrt(mod_rf_leafn_noPBR$prediction.error)
mod_rf_leafn_noPBR$r.squared
```

With hyperparameter tuning according to [this](https://bradleyboehmke.github.io/HOML/random-forest.html). No `PBR`.
```{r}
# create hyperparameter grid
hyper_grid <- expand.grid(
  mtry = floor(length(preds) * c(.1, .15, .25, .333, .4)),
  min.node.size = c(3, 5, 10), 
  replace = c(TRUE, FALSE),                               
  sample.fraction = c(.5, .7, .8),                       
  rmse = NA                                               
)

# execute full cartesian grid search
for(i in seq_len(nrow(hyper_grid))) {
  # fit model for ith hyperparameter combination
  fit <- ranger(
    formula = leafN ~ ., 
    data = dfs %>% 
      ungroup() %>% 
      dplyr::select(leafN, preds) %>% 
      dplyr::select(-PBR) %>% 
      drop_na(),
    num.trees       = length(preds) * 10,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$min.node.size[i],
    replace         = hyper_grid$replace[i],
    sample.fraction = hyper_grid$sample.fraction[i],
    verbose         = FALSE,
    seed            = 123,
    respect.unordered.factors = 'order',
  )
  # export OOB error 
  hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}

# assess top 10 models
hyper_grid %>%
  arrange(rmse) %>%
  head(10)

## save the best combination
best_hyper <- hyper_grid %>% 
  arrange(rmse) %>% 
  slice(1)
```
### Caret Random Forest

Using the caret library with hyperparameter based on best results from above. This is useful to get CV results.
Slightly better results with imputation, instead of dropping data.
```{r}
## create generic formula for the model and define preprocessing steps
pp <- recipe(leafN ~ ., data = dplyr::select(dfs, leafN, preds)) %>%
  
  ## impute by median as part of the recipe
  step_impute_median(all_predictors())
  

traincotrlParams <- trainControl( 
  method="cv", 
  number=5, 
  verboseIter=FALSE,
  savePredictions = "final"
  )

tune_grid <- expand.grid( .mtry = best_hyper$mtry, 
                          .min.node.size = best_hyper$min.node.size,
                          .splitrule = "variance"
                          )

set.seed(1982)

mod_rf_caret_leafn <- train(
  pp,
  data            = dplyr::select(dfs, leafN, preds),
  metric          = "RMSE",
  method          = "ranger",
  tuneGrid        = tune_grid,
  trControl       = traincotrlParams,
  replace         = best_hyper$replace,
  sample.fraction = best_hyper$sample.fraction,
  num.trees       = 2000,        # boosted for the final model
  importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
  )

mod_rf_caret_leafn_noimpute <- train(
  leafN ~ .,
  data = dfs %>%
      dplyr::select(leafN, preds) %>%
      dplyr::select(-PBR) %>%
      drop_na(),
  metric          = "RMSE",
  method          = "ranger",
  tuneGrid        = tune_grid,
  trControl       = traincotrlParams,
  replace         = best_hyper$replace,
  sample.fraction = best_hyper$sample.fraction,
  num.trees       = 2000,        # boosted for the final model
  importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
  )

mod_rf_caret_leafn
mod_rf_caret_leafn_noimpute
```

Visualise cross-validation results using results from the best tuned model.
```{r}
## get predicted values from cross-validation resamples, take mean across repetitions
df_cv <- mod_rf_caret_leafn$pred %>% 
  as_tibble() %>% 
  dplyr::filter(mtry == mod_rf_caret_leafn$bestTune$mtry, 
                splitrule == mod_rf_caret_leafn$bestTune$splitrule, 
                min.node.size == mod_rf_caret_leafn$bestTune$min.node.size) %>%
  separate(Resample, into = c(NA, "Fold"), sep = "old") %>% 
  dplyr::rename(idx = rowIndex)
  # left_join(
  #   dfs %>%
  #     ungroup() %>%
  #     drop_na() %>%
  #     dplyr::select(leafN) %>%
  #     mutate(idx = seq(nrow(.))),
  #   by = "idx"
  #   ) %>%
  # dplyr::select(obs = leafN, mod = pred)

out <- df_cv %>% 
  rbeni::analyse_modobs2("pred", "obs", type = "heat")
out$gg +
  ylim(5,40) + xlim(5,40)
```

<!-- ### Caret Neural Network -->

<!-- XXX Doesn't work well. XXX -->

<!-- Using the caret library with hyperparameter based on best results from above. This is useful to get CV results. -->
<!-- ```{r} -->
<!-- traincotrlParams <- trainControl(  -->
<!--   method="cv",  -->
<!--   number=5,  -->
<!--   verboseIter=FALSE, -->
<!--   savePredictions = "final" -->
<!--   ) -->

<!-- tune_grid <- expand.grid( .size = c(12,15,20),  -->
<!--                           .decay = c(0.1, 0.05, 0.01, 0.005) -->
<!--                           )  -->

<!-- set.seed(1982) -->

<!-- mod_nn_caret_leafn <- train( -->
<!--   leafN ~ ., -->
<!--   data = dfs %>%  -->
<!--       ungroup() %>%  -->
<!--       dplyr::select(leafN, preds) %>%  -->
<!--       dplyr::select(-PBR) %>%  -->
<!--       drop_na(), -->
<!--   metric    = "RMSE", -->
<!--   method    = "nnet", -->
<!--   preProc   = c("center", "scale"), -->
<!--   tuneGrid  = tune_grid, -->
<!--   trControl = traincotrlParams, -->
<!--   na.action = na.omit, -->
<!--   trace     = FALSE -->
<!--   ) -->
<!-- mod_rf_caret_leafn -->
<!-- mod_rf_caret_leafn$finalModel -->
<!-- ``` -->

## Interpret model

Variable importance.

```{r}
p1 <- vip(mod_rf_caret_leafn$finalModel, num_features = 45, bar = FALSE)
p1
```

Partial dependence:
```{r}
pdp_pred <- function(object, newdata){
  results <- mean(predict(object, newdata))$predictions
  return(results)
}

out <- partial(mod_rf_caret_leafn$finalModel, 
        train = dfs %>% 
          ungroup() %>% 
          dplyr::select(leafN, preds) %>% 
          dplyr::select(-PBR) %>% 
          drop_na(),
        pred.var = "vcmax25",
        # pred.fun = pdp_pred,
        grid.resolution = 50
)
head(out)
autoplot(out, rug = TRUE, train = as.data.frame(dfs %>% 
          ungroup() %>% 
          drop_na() %>% 
          dplyr::select(leafN, preds)))
```

## Feature elimination

Backward feature selection based on CV results. Breaks once R2 falls below 0.45.
```{r}
## specify target variable (as above)
target <- 'leafN'

# This is the vector of candidate predictors to be added in the model. To begin with, consider all as candidates.
preds_candidate <- preds 

# predictors retained in the model from the previous step. To begin with, is all predictors
preds_retained <- preds

## work with lists as much as possible (more flexible!)
df_metrics <- data.frame()

## common train control params
traincotrlParams <- trainControl( 
  method="cv", 
  number=5, 
  verboseIter=FALSE,
  savePredictions = "final"
)

## outer loop for k predictors
for (k_index in 1:length(preds)){
  
  # rsq_candidates <- c()
  df_rsq_candidates <- data.frame()
  fit_candidates <- list()
  
  ## inner loop for single additional predictor
  for (ipred in preds_candidate){
    
    # variable vector (new variable + retained variables) used in regression
    preds_after_drop <- preds_retained[-which(preds_retained == ipred)]
    
    # define formulate with newly-added predictor
    forml  <- as.formula(paste( target, '~', paste(preds_after_drop, collapse = '+')))
    
    # fit random forest model
    ## create generic formula for the model and define preprocessing steps
    pp <- recipe(forml, data = dplyr::select(dfs, leafN, preds)) %>%
      step_impute_median(all_predictors())
    
    ## No actual tuning here
    tune_grid <- expand.grid( .mtry = floor((length(preds_after_drop)-1) / 3), 
                              .min.node.size = 5,
                              .splitrule = "variance")
    set.seed(1982)
    
    fit <- train(
      pp,
      data            = dplyr::select(dfs, leafN, all_of(preds_after_drop)),
      metric          = "RMSE",
      method          = "ranger",
      tuneGrid        = tune_grid,
      trControl       = traincotrlParams,
      replace         = FALSE,
      sample.fraction = 0.5,
      num.trees       = 2000,        # boosted for the final model
      importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
    )
    
    # add model object to list, and name the element according to the added variable
    fit_candidates[[ ipred ]] <- fit
    
    # record metrics for all candidates
    rsq <- fit$results$Rsquared
    df_rsq_candidates <- bind_rows(df_rsq_candidates, data.frame(pred = ipred, rsq = rsq))

  }
  
  ## get name of candidate predictor for which, when dropped, the model still achieved the highest R2.
  pred_drop <- df_rsq_candidates %>%  # when storing R2 in a data frame
    arrange(desc(rsq)) %>% 
    slice(1) %>% 
    pull(pred) %>% 
    as.character()
  
  rsq_new <- df_rsq_candidates %>%  # when storing R2 in a data frame
    arrange(desc(rsq)) %>% 
    slice(1) %>% 
    pull(rsq) %>% 
    as.numeric()
  
  print(paste("Dropping", pred_drop, " R2 = ", rsq_new))
  
  ## exit feature elimination once R2 drops below 0.45
  if (rsq_new < 0.45) break 
  
  ## drop next unnecessary predictor  
  preds_retained <- preds_retained[-which(preds_retained == pred_drop)]
  
  # record CV r2 of respective model
  df_metrics <- df_metrics %>% 
    bind_rows(
      data.frame( pred = pred_drop,
                  rsq = rsq_new
      )
    )
  
  # remove the selected variable from the candidate variable list
  preds_candidate <- preds_candidate[-which(preds_candidate == pred_drop)]

}

df_metrics
```